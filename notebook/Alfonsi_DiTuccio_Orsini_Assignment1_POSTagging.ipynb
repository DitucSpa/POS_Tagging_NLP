{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1: Alfonsi, Di Tuccio, Orsini - POS TAGGING\n"
      ],
      "metadata": {
        "id": "Vtw8mh8y2den"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, we tested four different neural architectures (BiLSTM/ GRU + Dense Layers) for PoS tagging. One important aspect of this task was the embedding of OOVs words, which we handled by choosing the mean of the word context embeddings when it was possible. Finally, the two best models according to the macro F1 score are the BiLSTM followed by a Dense layer (BiLSTM model) and the two-layer BiLSTM followed by a Dense Layer (ML-BiLSTM model), which both perform with an F1 score equal to 0.79.\n",
        "\n",
        "This notebook is organized in 14 chapters:\n",
        "- chapter 1: import libraries and set the random seed;\n",
        "- chapter 2: download of the dataset folder;\n",
        "- chapter 3: Train-Validation-Test split + preprocessing of the data;\n",
        "- chapter 4: some statistics about the data;\n",
        "- chapter 5: download of the GloVe embedding;\n",
        "- chapter 6: OOV vocabulary;\n",
        "- chapter 7: words embedding;\n",
        "- chapter 8: padding and truncation for X_train, X_val, X_test;\n",
        "- chapter 9: padding and truncation for y_train, y_val, y_test;\n",
        "- chapter 10: training of the models;\n",
        "- chapter 11: evaluation of the model on validation set;\n",
        "- chapter 12: BiLSTM model performance on test set;\n",
        "- chapter 13: ML-BiLSTM model performance on test set;\n",
        "- chapter 14: Bibliography and sitography"
      ],
      "metadata": {
        "id": "FdwsJKA-7k9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Random Seed"
      ],
      "metadata": {
        "id": "g10GGVj9hlFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from numpy.random import seed\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU, Input, Masking, TimeDistributed, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "def set_reproducibility(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "\n",
        "# at the end of the notebook, you'll be able to see results using different seeds\n",
        "random_seed = 10\n",
        "set_reproducibility(random_seed)"
      ],
      "metadata": {
        "id": "IbD6ZwAjhjPq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY0MAkV_8PlE"
      },
      "source": [
        "## 2. Download of the dataset folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHVbocdj8Gzq",
        "outputId": "e65f6fab-1768-48b2-f98f-319490f72c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import urllib\n",
        "    \n",
        "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
        "\n",
        "data_path = os.path.join(os.getcwd(),\"\")\n",
        "data_zip = os.path.join(os.getcwd(),\"\", \"dependency_treebank.zip\")\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    os.makedirs(data_path)\n",
        "\n",
        "if not os.path.exists(data_zip):\n",
        "    urllib.request.urlretrieve(url, data_zip)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(data_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path=data_path)\n",
        "    print(\"Successful extraction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH089Sbn8a-f"
      },
      "source": [
        "## 3. Train-Validation-Test split + preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4bb3nGIzLXG"
      },
      "source": [
        "First we upload all the .dp file names in the folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hmcNyTp78YWB"
      },
      "outputs": [],
      "source": [
        "path = \"dependency_treebank\"\n",
        "filenames = []\n",
        "for file in os.listdir(\"dependency_treebank\"):\n",
        "    if file.endswith(\".dp\"):\n",
        "        filenames.append(file)\n",
        "filenames.sort()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p68fQWOJzVHI"
      },
      "source": [
        "Each document of the dataset contains a column with the words of the sentences, while a second column contains the corresponding POS tags for each word. The dataset contains also a third column, which was ignored during this assignment according to the instructions. Since the first two columns are separated by a tabulation, we kept the correspondence tag for each word and then separated each sentence with the occurrence of a single dot. When each word is extracted by the document, we applied a preprocessing step by reducing all the characters to lowercase, while the POS tags for curly and round brackets are substituted by their punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aGr4qJsdxw0d"
      },
      "outputs": [],
      "source": [
        "def preprocessing(text):\n",
        "  return text.replace('-LRB-', '(').replace('-RRB-', ')').replace('-LCB-', '{').replace('-RCB-', '}').lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shHYRaMTznYx"
      },
      "source": [
        "Then, we upload the data into a single dataframe with:\n",
        "- Words (each sentence is stored in the words column; so, we've splitted into sentences instead documents);\n",
        "- Tags (the tags for the sentence);\n",
        "- Document (the document where we can find the sentence);\n",
        "- Split (if the sentence is in the training, val or test set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SiHM9z-Q8eaS",
        "outputId": "4c74c45c-8460-458a-dd38-212d2c564ec7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Words  \\\n",
              "0  pierre vinken , 61 years old , will join the b...   \n",
              "1  mr. vinken is chairman of elsevier n.v. , the ...   \n",
              "2  rudolph agnew , 55 years old and former chairm...   \n",
              "3  a form of asbestos once used to make kent ciga...   \n",
              "\n",
              "                                                Tags  Document  Split  \n",
              "0  nnp nnp , cd nns jj , md vb dt nn in dt jj nn ...         1  train  \n",
              "1        nnp nnp vbz nn in nnp nnp , dt nnp vbg nn .         1  train  \n",
              "2  nnp nnp , cd nns jj cc jj nn in nnp nnp nnp nn...         2  train  \n",
              "3  dt nn in nn rb vbn to vb nnp nn nns vbz vbn dt...         3  train  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc22ba8e-41b0-41b3-bb74-d7a32cef5cc6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Words</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Document</th>\n",
              "      <th>Split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pierre vinken , 61 years old , will join the b...</td>\n",
              "      <td>nnp nnp , cd nns jj , md vb dt nn in dt jj nn ...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mr. vinken is chairman of elsevier n.v. , the ...</td>\n",
              "      <td>nnp nnp vbz nn in nnp nnp , dt nnp vbg nn .</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>rudolph agnew , 55 years old and former chairm...</td>\n",
              "      <td>nnp nnp , cd nns jj cc jj nn in nnp nnp nnp nn...</td>\n",
              "      <td>2</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a form of asbestos once used to make kent ciga...</td>\n",
              "      <td>dt nn in nn rb vbn to vb nnp nn nns vbz vbn dt...</td>\n",
              "      <td>3</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc22ba8e-41b0-41b3-bb74-d7a32cef5cc6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cc22ba8e-41b0-41b3-bb74-d7a32cef5cc6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cc22ba8e-41b0-41b3-bb74-d7a32cef5cc6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# we've stored all the sentences in a list\n",
        "# then the list is converted to a dataframe\n",
        "# we can't use pd.concat in the loop (O(pd.concat) > O(list.append))\n",
        "df_list = []\n",
        "\n",
        "split = {'train':100, 'validation':150}\n",
        "for i in range(len(filenames)):\n",
        "  with open(os.path.join(path, filenames[i]), 'r') as f:\n",
        "    words = []\n",
        "    tags = []\n",
        "    for line in f:\n",
        "      try:\n",
        "        # we split each line using the tabulation\n",
        "        tmp = line.split(\"\\t\")\n",
        "\n",
        "        # if the line contains only the word or white space without tags, raise error\n",
        "        tags.append(preprocessing(tmp[1]))\n",
        "        words.append(preprocessing(tmp[0]))\n",
        "        \n",
        "        # if the word is exactly a point, the sentence is over\n",
        "        if tmp[0] == \".\":\n",
        "          split_df = 'test'\n",
        "          if i < split['train']: split_df = 'train'\n",
        "          elif i < split['validation']: split_df = 'validation'\n",
        "\n",
        "          # we use a white space between each words and tags\n",
        "          data = {\"Words\":' '.join(words), \"Tags\":' '.join(tags), 'Document':i+1, 'Split':split_df}\n",
        "          df_list.append(data)\n",
        "          words = []\n",
        "          tags = []\n",
        "      \n",
        "      # if the line contains only white space\n",
        "      except:\n",
        "        pass\n",
        "df = pd.DataFrame(df_list)\n",
        "df.head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMkxOXYj12Lf"
      },
      "source": [
        "After that, we split the dataframe into train, test and validation according to the instructions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tm5nZjNj8jxy",
        "outputId": "ce90809d-0b12-44de-e38d-b07501d0f940"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  Words  \\\n",
              "3192  intelogic trace inc. , san antonio , texas , s...   \n",
              "3193  the move boosts intelogic chairman asher edelm...   \n",
              "3194  mr. ackerman already is seeking to oust mr. ed...   \n",
              "3195  the action followed by one day an intelogic an...   \n",
              "\n",
              "                                                   Tags  Document Split  \n",
              "3192  nnp nnp nnp , nnp nnp , nnp , vbd prp vbd cd c...       151  test  \n",
              "3193  dt nn vbz nnp nnp nnp nnp pos nn to cd nn in c...       151  test  \n",
              "3194  nnp nnp rb vbz vbg to vb nnp nnp in nn in nnp ...       151  test  \n",
              "3195  dt nn vbn in cd nn dt nnp nn in prp md vb dt n...       151  test  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c73e0d5d-0787-4837-b23e-f8d5419c1d82\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Words</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Document</th>\n",
              "      <th>Split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3192</th>\n",
              "      <td>intelogic trace inc. , san antonio , texas , s...</td>\n",
              "      <td>nnp nnp nnp , nnp nnp , nnp , vbd prp vbd cd c...</td>\n",
              "      <td>151</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3193</th>\n",
              "      <td>the move boosts intelogic chairman asher edelm...</td>\n",
              "      <td>dt nn vbz nnp nnp nnp nnp pos nn to cd nn in c...</td>\n",
              "      <td>151</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3194</th>\n",
              "      <td>mr. ackerman already is seeking to oust mr. ed...</td>\n",
              "      <td>nnp nnp rb vbz vbg to vb nnp nnp in nn in nnp ...</td>\n",
              "      <td>151</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3195</th>\n",
              "      <td>the action followed by one day an intelogic an...</td>\n",
              "      <td>dt nn vbn in cd nn dt nnp nn in prp md vb dt n...</td>\n",
              "      <td>151</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c73e0d5d-0787-4837-b23e-f8d5419c1d82')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c73e0d5d-0787-4837-b23e-f8d5419c1d82 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c73e0d5d-0787-4837-b23e-f8d5419c1d82');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train = df[df.Split=='train']\n",
        "validation = df[df.Split=='validation']\n",
        "test = df[df.Split=='test']\n",
        "test.head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZrLvFqj2a-a"
      },
      "source": [
        "## 4. Some Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F52c1w2K2fgS"
      },
      "source": [
        "Before going into deep with the vocabulary and Glove, we saw the distribution of the length of each sentece of the training set, since in a real world scenario it is the only set of data available to train the model. In particular, the histogram below shows that the great majority of the sentences has length between 2 and 50, while only few sentences has length between 50 and 250 (which is the max length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lHwIUYRCLdvx",
        "outputId": "16ad9a17-7eb6-4dc8-c121-f436d18a32af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f53cef527c0>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "pd.DataFrame(train.Words.apply(lambda x: x.split(\" \")).apply(len)).hist(grid=True, bins=60, figsize=(10,7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-NAkos-3yML"
      },
      "source": [
        "Now we compute some statistics: as we can see from the following code, there are 1936 sentences, the mean is about 24.5 words and the max is 250. Since the model requires a fixed input length, it is important to choose it very carefully:\n",
        "\n",
        "- taking the highest value means doing a lot of padding (and here we have only two sentences with over 100 words;\n",
        "- taking the mean value means to truncate the half of the sentences.\n",
        "In the end, we've decided to take the 75% percentile (a compromise between the max and the mean)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pEY9wDho3NN-",
        "outputId": "8abbf339-abab-4092-bacf-dc38cafc1114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 75% quantile is:  31 words\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Words\n",
              "count  1936.000000\n",
              "mean     24.459194\n",
              "std      12.686699\n",
              "min       2.000000\n",
              "25%      16.000000\n",
              "50%      23.000000\n",
              "75%      31.000000\n",
              "max     250.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-754daaac-3a3d-4575-b9e1-d1e6c1835128\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1936.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>24.459194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>12.686699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>16.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>23.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>31.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>250.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-754daaac-3a3d-4575-b9e1-d1e6c1835128')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-754daaac-3a3d-4575-b9e1-d1e6c1835128 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-754daaac-3a3d-4575-b9e1-d1e6c1835128');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "max_length = int(pd.DataFrame(train.Words.apply(lambda x: x.split(\" \")).apply(len)).quantile(0.75))\n",
        "print(\"The 75% quantile is: \", max_length, 'words')\n",
        "pd.DataFrame(train.Words.apply(lambda x: x.split(\" \")).apply(len)).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lEM0m_L49Vg"
      },
      "source": [
        "Then, we plot the distribution of each tag for each split. Here we can see that special chars appear few times (except for the $ symbol, because there are some currency values). The most common tag is the noun tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IclzGwK8aLrF",
        "outputId": "72c23442-cd00-46a4-e99a-09adef41fbf3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f53ce733430>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(20, 7))\n",
        "\n",
        "tags_train = []\n",
        "tags_val = []\n",
        "tags_test = []\n",
        "\n",
        "for i in train.Tags.apply(lambda x: x.split(\" \")):\n",
        "  for j in i:\n",
        "    tags_train.append(j)\n",
        "\n",
        "for i in validation.Tags.apply(lambda x: x.split(\" \")):\n",
        "  for j in i:\n",
        "    tags_val.append(j)\n",
        "\n",
        "for i in test.Tags.apply(lambda x: x.split(\" \")):\n",
        "  for j in i:\n",
        "    tags_test.append(j)\n",
        "\n",
        "pd.DataFrame(tags_train).value_counts().plot(kind='bar', figsize=(20,7), label = 'train', color = 'blue')\n",
        "pd.DataFrame(tags_val).value_counts().plot(kind='bar', figsize=(20,7), label = 'val', color = 'orange')\n",
        "pd.DataFrame(tags_test).value_counts().plot(kind='bar', figsize=(20,7), label = 'test', color = 'green')\n",
        "\n",
        "plt.title('Tags distribution over train, validation and test set')\n",
        "plt.xlabel('Tags')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the plot, each class has approximately the same distribution for each split, which however has an unbalanced distribution between the classes.\n"
      ],
      "metadata": {
        "id": "ixbbaxGauKdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Unique tags in train: ', len(set(tags_train)))\n",
        "print('Unique tags in val: {0}, tags: {1}'.format(len(set(tags_val)), ', '.join([k.upper() for k in set(tags_train) if not k in set(tags_val)])))\n",
        "print('Unique tags in test: {0}, tags: {1}'.format(len(set(tags_test)), ', '.join([k.upper() for k in set(tags_train) if not k in set(tags_test)])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiuOVtKa6ugY",
        "outputId": "5437427d-5e88-4664-aed4-cc6ae7156fce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tags in train:  45\n",
            "Unique tags in val: 44, tags: SYM\n",
            "Unique tags in test: 40, tags: LS, FW, SYM, #, UH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some classes don't appear in the validation and test split: SYM for both, while #, UH, LS and FW for the test."
      ],
      "metadata": {
        "id": "jlOsRV0v-faF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBvJbVok8uh0"
      },
      "source": [
        "## 5. Download of Glove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba8EZELf5-qn"
      },
      "source": [
        "We've used Wikipedia 2014 + Gigaword 5 Glove (6B tokens, 400k vocab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpJ5iTye81C_",
        "outputId": "3db41d68-8013-4ea8-fc39-ee6385cc9eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ],
      "source": [
        "url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "\n",
        "glove_path = os.path.join(os.getcwd(),\"Glove\")\n",
        "glove_zip = os.path.join(os.getcwd(),\"Glove\", \"glove.6B.zip\")\n",
        "\n",
        "if not os.path.exists(glove_path):\n",
        "    os.makedirs(glove_path)\n",
        "\n",
        "if not os.path.exists(glove_zip):\n",
        "    urllib.request.urlretrieve(url, glove_zip)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(glove_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path=glove_path)\n",
        "    print(\"Successful extraction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BfCOTIX6hm1"
      },
      "source": [
        "Here, we've written the function for getting the vocabulary of Glove (given the number of columns, default 50)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vWs7e8y587sg"
      },
      "outputs": [],
      "source": [
        "def build_glove_vocabulary(columns=50):\n",
        "  glove_file = os.path.join(os.getcwd(),\"Glove\", \"glove.6B.{0}d.txt\".format(columns))\n",
        "  print (\"Loading Glove Model\")\n",
        "  with open(glove_file, encoding=\"utf8\" ) as f:\n",
        "      lines = f.readlines()\n",
        "  glove_vocabulary = {}\n",
        "  for line in lines:\n",
        "      splits = line.split()\n",
        "      glove_vocabulary[splits[0]] = np.array([float(val) for val in splits[1:]])\n",
        "  print (\"Done.\",len(glove_vocabulary.keys()),\" words loaded!\")\n",
        "  return glove_vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxyVGB5b9fA6"
      },
      "source": [
        "## 6. OOV Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyJRAH3w7F2I"
      },
      "source": [
        "One of the most important aspect for RNN is the embedding phase. First, we add the OOVs in the Glove vocabulary. In a real scenario, we:\n",
        "\n",
        "*   Start with a vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "*   Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split\n",
        "*   Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "*   Training of the model(s)\n",
        "*   Compute embeddings for terms OOV2 of the validation split\n",
        "*   Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "*   Validation of the model(s)\n",
        "*   Compute embeddings for terms OOV3 of the test split\n",
        "*   Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "*   Testing of the final mode<br>\n",
        "\n",
        "In this case we already have all the documents and so we have simplified a little bit the previous procedure (as explained in the instructions). In particular, we have added the OOVs and calculated embeddings at each step (first train, then val and finally test), but the model is trained and tested after evaluating the OOVs for train, val and test. For the Embedding of the OOVs, we have used the mean of the embedding of the context for each OOV word when it was possible, otherwise we took a random vector. To choose the best embedding dimension of GloVe and the best context size we tested some configuration using the first model (BiLSTM + Dense Layer) on the validation set, the best results were with: \n",
        "\n",
        "- 100 dimension (Glove);\n",
        "- 3 context (actual_word - 3, actual_word + 3)\n",
        "\n",
        "First, we define a function to build the vocabulary of the dataframe using the tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v-1nAl_hE0mE"
      },
      "outputs": [],
      "source": [
        "# the words are sep. by a white space, so we simply use split=' '\n",
        "# X_column --> the name of the columns where the sentences are stored in the dataset (in this case Words)\n",
        "def build_vocabulary(dataframe, X_column):\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(split=' ', num_words=0, filters='\\t\\n')\n",
        "  tokenizer.fit_on_texts(dataframe[X_column])\n",
        "  return tokenizer.word_index, tokenizer.index_word"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need a functoin to find the OOVs of a 'new_vocabulary' wrt 'start_vocabulary':"
      ],
      "metadata": {
        "id": "fe6gz6zLRyWi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "97-LokX5ILrT"
      },
      "outputs": [],
      "source": [
        "def find_OOV(start_vocabulary, new_vocabulary):\n",
        "  return set(new_vocabulary).difference(set(start_vocabulary.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And a function to build word-word co-occurrence matrix based on word counts:"
      ],
      "metadata": {
        "id": "X25EloRYR-ri"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GqgAaiWW_M1Q"
      },
      "outputs": [],
      "source": [
        "# from tutorial2 --> calculate the co-occurrence\n",
        "def co_occurrence_count(df, idx_to_word, word_to_idx, window_size=4) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds word-word co-occurrence matrix based on word counts.\n",
        "\n",
        "    :param df: pre-processed dataset (pandas.DataFrame)\n",
        "    :param idx_to_word: vocabulary map (index -> word) (dict)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "\n",
        "    :return\n",
        "      - co_occurrence symmetric matrix of size |V| x |V| (|V| = vocabulary size)\n",
        "    \"\"\"\n",
        "    vocab_size = len(idx_to_word)+1\n",
        "    co_occurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "    \n",
        "    for sentence in df.Words.values:\n",
        "        tokens = sentence.split()\n",
        "        for pos, token in enumerate(tokens):\n",
        "            start = max(0, pos - window_size)\n",
        "            end = min(pos + window_size + 1, len(tokens))\n",
        "\n",
        "            first_word_index = word_to_idx[token]\n",
        "\n",
        "            for pos2 in range(start, end):\n",
        "                if pos2 != pos:\n",
        "                    second_token = tokens[pos2]\n",
        "                    second_word_index = word_to_idx[second_token]\n",
        "                    co_occurrence_matrix[first_word_index, second_word_index] += 1\n",
        "    return co_occurrence_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we define a function to concatenate the OOVs to the initial vocabulary:"
      ],
      "metadata": {
        "id": "xBX1nV66SQ38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZGtdQcQjG7AB"
      },
      "outputs": [],
      "source": [
        "# initial_vocabulary --> the vocabulary where we want to add OOVs\n",
        "# OOV --> a list of OOVs\n",
        "# word_idx_vocab_with_OOV --> the vocabulary with word as key with its index (including OOV)\n",
        "# idx_word_vocab_with_OOV --> the vocabulary with index as key with its word (including OOV)\n",
        "# co_occurrence --> the co-occurrence-matrix for the given dataframe (the vocabulary are word_idx_vocab_with_OOV and idx_word_vocab_with_OOV)\n",
        "def mean_concat_OOV_vocabulary(initial_vocabulary, OOV, word_idx_vocab_with_OOV, idx_word_vocab_with_OOV, co_occurrence, columns=50):\n",
        "  for word in OOV:\n",
        "    encode = word_idx_vocab_with_OOV[word]\n",
        "    row_occurence = co_occurrence[encode,:]\n",
        "\n",
        "    # if the word hasn't any context --> random embedding\n",
        "    if np.all((row_occurence == 0)):\n",
        "      embedding_vector = np.random.uniform(low=-0.05, high=0.05, size = columns)\n",
        "    \n",
        "    # if the word has some context, we calculate the mean of the embedding (in the initial_vocabulary) of the contexts\n",
        "    else:\n",
        "      weights = row_occurence[row_occurence!=0]\n",
        "      contexts = [idx_word_vocab_with_OOV[c] for c in np.where(row_occurence != 0)[0]]\n",
        "      \n",
        "      # if a context word is a OOV unseen, we random it\n",
        "      embeddings = [initial_vocabulary.get(context_word, np.random.uniform(low=-0.05, high=0.05, size = columns)) for context_word in contexts]\n",
        "      embedding_vector = np.average(embeddings, weights=weights, axis=0)\n",
        "    initial_vocabulary[word] = embedding_vector\n",
        "\n",
        "  return initial_vocabulary\n",
        "\n",
        "def random_concat_OOV_vocabulary(initial_vocabulary, OOV, columns=50):\n",
        "  for word in OOV:\n",
        "    initial_vocabulary[word] = np.random.uniform(low=-0.05, high=0.05, size=columns)\n",
        "  return initial_vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soaXgwWy-7nh"
      },
      "source": [
        "To build the vocabulary, we use a sequential order (easy to understand), instead using some functions and having automated code. We remember that we started with a vocabulary that contains the words of the training set that appear also in GloVe, and then we added the OOVs of the training set. Then this process was repeated also for the validation set, where the OOVs are considered wrt the GloVe vocabulary plus the training OOVs; in the same way we operate with the test set, wrt to the GloVe vocabulary plus the OOVs of training and validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JH6IuRNEdj2",
        "outputId": "33d96913-ebe8-4f35-ac08-c95ef800f589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Glove Model\n",
            "Done. 400000  words loaded!\n",
            "\n",
            "The vocabulary (tokenizer) starts from 1 with the encode (0 reserved to -PAD-, see later), so the shape for the co-occ is length+1.\n",
            "\n",
            "The len of Train Vocabulary is: 7402\n",
            "The OOVs for Train wrt Glove are: 355\n",
            "The percentage of train OOVs is: 5%\n",
            "The co-occ-matrix for Train has a shape of: (7403, 7403)\n",
            "The new vocabulary size is: 400355\n",
            "\n",
            "The len of Validation Vocabulary is: 5420\n",
            "The OOVs for Validation wrt Glove + Train's OOV are: 189\n",
            "The percentage of validation OOVs is: 3%\n",
            "The co-occ-matrix for Validation has a shape of: (5421, 5421)\n",
            "The new vocabulary size is: 400544\n",
            "\n",
            "The len of Test Vocabulary is: 3407\n",
            "The OOVs for Test wrt Glove + Train's OOV + Val's OOV are: 128\n",
            "The percentage of test OOVs is: 4%\n",
            "The co-occ-matrix for Test has a shape of: (3408, 3408)\n",
            "The new vocabulary size is: 400672\n"
          ]
        }
      ],
      "source": [
        "# we have found 100 as best dimension for Glove\n",
        "dimension = 100\n",
        "context_word = 1\n",
        "\n",
        "embedding_function = {'mean':mean_concat_OOV_vocabulary, 'random':random_concat_OOV_vocabulary}\n",
        "current_embedding = 'mean' # use 'mean' for a mean context embedding, 'random' otherwise\n",
        "\n",
        "# ------------GLOVE------------\n",
        "# build the glove voc (400k voc)\n",
        "glove_vocabulary = build_glove_vocabulary(columns=dimension)\n",
        "print()\n",
        "print('The vocabulary (tokenizer) starts from 1 with the encode (0 reserved to -PAD-, see later), so the shape for the co-occ is length+1.')\n",
        "print()\n",
        "\n",
        "\n",
        "# ------------TRAIN------------\n",
        "\n",
        "# create the vocabulary for the train set\n",
        "train_vocabulary_w_i, train_vocabulary_i_w = build_vocabulary(train, \"Words\")\n",
        "print(\"The len of Train Vocabulary is: {0}\".format(len(train_vocabulary_w_i)))\n",
        "\n",
        "# find the OOV for the train wrt the Glove\n",
        "train_OOV = find_OOV(glove_vocabulary, train_vocabulary_w_i)\n",
        "print(\"The OOVs for Train wrt Glove are: {0}\".format(len(train_OOV)))\n",
        "\n",
        "print(\"The percentage of train OOVs is: {:.0%}\".format(round(len(train_OOV)/len(train_vocabulary_w_i),2)))\n",
        "# build the co_occurrence_matrix for train using a window_size of 3\n",
        "co_occurrence_train = co_occurrence_count(train, train_vocabulary_i_w, train_vocabulary_w_i, window_size=context_word)\n",
        "print(\"The co-occ-matrix for Train has a shape of: {0}\".format(co_occurrence_train.shape))\n",
        "\n",
        "# update the Glove vocabulary with the train's OOV\n",
        "train_glove_vocabulary = embedding_function[current_embedding](glove_vocabulary, train_OOV, train_vocabulary_w_i, train_vocabulary_i_w, co_occurrence_train, columns=dimension)\n",
        "print(\"The new vocabulary size is: {0}\".format(len(train_glove_vocabulary)))\n",
        "print()\n",
        "\n",
        "\n",
        "# ------------VALIDATION------------\n",
        "\n",
        "# create the vocabulary for the validation set\n",
        "validation_vocabulary_w_i, validation_vocabulary_i_w = build_vocabulary(validation, \"Words\")\n",
        "print(\"The len of Validation Vocabulary is: {0}\".format(len(validation_vocabulary_w_i)))\n",
        "\n",
        "# find the OOV for the validation set wrt the Glove + Train's OOV\n",
        "validation_OOV = find_OOV(train_glove_vocabulary, validation_vocabulary_w_i)\n",
        "print(\"The OOVs for Validation wrt Glove + Train's OOV are: {0}\".format(len(validation_OOV)))\n",
        "print(\"The percentage of validation OOVs is: {:.0%}\".format(round(len(validation_OOV)/len(validation_vocabulary_w_i),2)))\n",
        "\n",
        "# build the co_occurrence_matrix for val using a window_size of 3\n",
        "co_occurrence_validation = co_occurrence_count(validation, validation_vocabulary_i_w, validation_vocabulary_w_i, window_size=context_word)\n",
        "print(\"The co-occ-matrix for Validation has a shape of: {0}\".format(co_occurrence_validation.shape))\n",
        "\n",
        "# update the Glove + Train's OOV vocabulary with the validations's OOV\n",
        "train_val_glove_vocabulary = embedding_function[current_embedding](train_glove_vocabulary, validation_OOV, validation_vocabulary_w_i, validation_vocabulary_i_w, co_occurrence_validation, columns=dimension)\n",
        "print(\"The new vocabulary size is: {0}\".format(len(train_val_glove_vocabulary)))\n",
        "print()\n",
        "\n",
        "\n",
        "# ------------TEST------------\n",
        "\n",
        "# create the vocabulary for the test set\n",
        "test_vocabulary_w_i, test_vocabulary_i_w = build_vocabulary(test, \"Words\")\n",
        "print(\"The len of Test Vocabulary is: {0}\".format(len(test_vocabulary_w_i)))\n",
        "\n",
        "# find the OOV for the test set wrt the Glove + Train's OOV + Val's OOV\n",
        "test_OOV = find_OOV(train_val_glove_vocabulary, test_vocabulary_w_i)\n",
        "print(\"The OOVs for Test wrt Glove + Train's OOV + Val's OOV are: {0}\".format(len(test_OOV)))\n",
        "print(\"The percentage of test OOVs is: {:.0%}\".format(round(len(test_OOV)/len(test_vocabulary_w_i),2)))\n",
        "# build the co_occurrence_matrix for val using a window_size of 3\n",
        "co_occurrence_test = co_occurrence_count(test, test_vocabulary_i_w, test_vocabulary_w_i, window_size=context_word)\n",
        "print(\"The co-occ-matrix for Test has a shape of: {0}\".format(co_occurrence_test.shape))\n",
        "\n",
        "# update the Glove + Train's OOV vocabulary with the validations's OOV\n",
        "train_val_test_glove_vocabulary = embedding_function[current_embedding](train_val_glove_vocabulary, test_OOV, test_vocabulary_w_i, test_vocabulary_i_w, co_occurrence_test, columns=dimension)\n",
        "print(\"The new vocabulary size is: {0}\".format(len(train_val_test_glove_vocabulary)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbnbJoymAv0X",
        "outputId": "cabb8f6a-544e-4167-a833-e172e9a14cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'more-advanced', 'crookery', 'computer-services', '566.54', '630.9', '811.9', 'protein-1', 'weapons-modernization', '170,262', 'inter-tel', 'blood-cell', '5,699', '494.50', 'asset-valuation', '87-store', 'acquisition-minded', 'intelogic', 'constitutional-law', '36-store', '608,413', 'ex-dividend', '120-a-share', 'cash-and-stock', 'waymar', 'pro-iranian', 'disputada', 'passenger-car', 'bankruptcy-law', '292.32', '26,956', 'reupke', '387.8', 'prior-year', '2,050-passenger', 'blue-chips', 'yet-to-be-formed', 'unicorp', '377.60', '237-seat', 'tete-a-tete', '263.07', '319.75', '220.45', 'nofzinger', '50\\\\/50', 'g.m.b', 'several-year', 'conn.based', 'high-rolling', '226,570,380', 'near-limit', 'above-market', 'headcount-control', '5.2180', '434.4', 'sewing-machine', 'lentjes', '158,666', 'waertsilae', 'cost-control', 'seven-yen', 'per-share', 'bread-and-butter', '372.14', 'copper-rich', '154,240,000', '618.1', '361.8', '734.9', 'corn-buying', '11-month-old', '45-a-share', '967,809', 'yoshihashi', 'heavy-truck', '55-a-share', 'newspaper-printing', '341.20', 'bronces', '300-day', 'diloreto', 'exxon-owned', 'sept.30', 'guber\\\\/peters', '6,799', 'garden-variety', 'life-of-contract', 'cents-a-unit', 'shareholder-rights', '188.84', '40-megabyte', '1206.26', '19-month-old', '131.01', '43.875', '126.15', '11,390,000', 'early-retirement', '0.0085', 'colorliner', 'noncompetitively', '129.91', '1.637', 'manmade-fiber', 'derchin', 'hasbrouk', '38.875', 'interleukin-3', 'louisiana-pacific', '18-a-share', 'mariotta', 'minimum-wage', 'johnson-era', '1.1650', 'disputado', 'information-services', 'arbitrage-related', '34.625', 'lobsenz', '1.916', 'staff-reduction', 'launch-vehicle', 'txo', '142.84', '3436.58', 'hadson', '83,206', '100-megabyte'}\n"
          ]
        }
      ],
      "source": [
        "# some OOVs of the test set (in particular acronyms and numbers)\n",
        "print(test_OOV)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we do the same with random assignment for the OOVs. As we can see, the vocabulary has 400672 elements as before, with random embedding for the OOVs."
      ],
      "metadata": {
        "id": "9C7zR5jnCC7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_split(dataset, vocabulary, column_name=\"Words\", window_size=3, dimension=50, current_embedding=random_concat_OOV_vocabulary):\n",
        "  vocabulary_w_i, vocabulary_i_w = build_vocabulary(dataset, column_name)\n",
        "  OOV = find_OOV(vocabulary, vocabulary_w_i)\n",
        "  return current_embedding(vocabulary, OOV, columns=dimension)\n",
        "\n",
        "random_vocabulary = glove_vocabulary.copy()\n",
        "random_vocabulary = build_split(train, random_vocabulary, dimension=dimension)\n",
        "random_vocabulary = build_split(validation, random_vocabulary, dimension=dimension)\n",
        "random_vocabulary = build_split(test, random_vocabulary, dimension=dimension)\n",
        "print(len(random_vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ked5GYWk__i8",
        "outputId": "29fcebaa-49c5-44dc-e6d8-a66a144277ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE126aDp4TIY"
      },
      "source": [
        "## 7. Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yjr2we7nBVwQ"
      },
      "source": [
        "Now we have a vocabulary without OOVs of train, val and test. So, we need to build the Embedding Matrix. For simplicity, we build an embedding matrix with only the words of training, validation and test set, without considering the other words of GloVe that don't appear in this task. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "28f4Rco5OLhA"
      },
      "outputs": [],
      "source": [
        "def build_embedding_matrix(voc_embedding, vocabulary, columns=50):\n",
        "    rows = len(vocabulary)\n",
        "    embedding_matrix = np.zeros((rows+1, columns))\n",
        "    for word, index in vocabulary.items():\n",
        "      embedding_vector = voc_embedding.get(word)\n",
        "      embedding_matrix[index] = embedding_vector\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiPc2X8SNsF0",
        "outputId": "c483cc1c-0f77-4908-ede4-af9ea4394c10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10946, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# ------------TRAIN-VAL-TEST------------\n",
        "# build embedding matrix using the Glove Voc + OOVs of train+val+test (computed previously)\n",
        "train_test_val_vocabulary_w_i, train_test_val_vocabulary_i_w = build_vocabulary(df, \"Words\")\n",
        "embedding_matrix = build_embedding_matrix(train_val_test_glove_vocabulary, train_test_val_vocabulary_w_i, columns=dimension)\n",
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE9CNutqIM0a"
      },
      "source": [
        "The Embedding Matrix (train+val+test) is: (without the first row, -PAD- with only 0s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "FXaPKNmcHwMY",
        "outputId": "9e7398c1-cbf1-43fb-fc04-99c2381489fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0        1        2        3         4         5         6   \\\n",
              "0      0.000000  0.00000  0.00000  0.00000  0.000000  0.000000  0.000000   \n",
              "1     -0.107670  0.11053  0.59812 -0.54361  0.673960  0.106630  0.038867   \n",
              "2     -0.038194 -0.24487  0.72812 -0.39961  0.083172  0.043953 -0.391410   \n",
              "3     -0.339790  0.20941  0.46348 -0.64792 -0.383770  0.038034  0.171270   \n",
              "4     -0.152900 -0.24279  0.89837  0.16996  0.535160  0.487840 -0.588260   \n",
              "...         ...      ...      ...      ...       ...       ...       ...   \n",
              "10941 -0.674900  1.24910 -0.20589  0.30409  0.956890 -0.070804 -0.739220   \n",
              "10942  0.353960 -1.42890  0.74630 -0.20343 -0.580970  0.712080 -0.156970   \n",
              "10943  0.168650  1.19110  0.17452 -0.27608 -0.265470 -0.307260  0.745120   \n",
              "10944 -0.324430  0.84650 -0.56920 -0.37743  1.048800 -1.067600 -0.334530   \n",
              "10945  0.309570 -0.30719  0.42018 -0.88324  0.138400 -0.130830  0.034690   \n",
              "\n",
              "             7         8         9   ...        90        91        92  \\\n",
              "0      0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "1      0.354810  0.063510 -0.094189  ...  0.349510 -0.722600  0.375490   \n",
              "2      0.334400 -0.575450  0.087459  ...  0.016215 -0.017099 -0.389840   \n",
              "3      0.159780  0.466190 -0.019169  ... -0.063351 -0.674120 -0.068895   \n",
              "4     -0.179820 -1.358100  0.425410  ...  0.187120 -0.018488 -0.267570   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "10941  0.132180  0.625330  0.068480  ...  0.267230 -0.344820 -0.247260   \n",
              "10942  0.330540 -0.120700  0.820530  ... -1.162800 -0.129670 -0.301680   \n",
              "10943  0.485540 -0.078094 -0.079655  ... -0.547290  0.382220 -0.207870   \n",
              "10944  0.049853  0.732600  0.238410  ...  0.086618  0.429520 -0.434800   \n",
              "10945  1.586300  0.611050  0.484240  ... -0.000066  0.099137  0.531750   \n",
              "\n",
              "             93       94       95       96        97       98        99  \n",
              "0      0.000000  0.00000  0.00000  0.00000  0.000000  0.00000  0.000000  \n",
              "1      0.444100 -0.99059  0.61214 -0.35111 -0.831550  0.45293  0.082577  \n",
              "2      0.874240 -0.72569 -0.51058 -0.52028 -0.145900  0.82780  0.270620  \n",
              "3      0.536040 -0.87773  0.31802 -0.39242 -0.233940  0.47298 -0.028803  \n",
              "4      0.727000 -0.59363 -0.34839 -0.56094 -0.591000  1.00390  0.206640  \n",
              "...         ...      ...      ...      ...       ...      ...       ...  \n",
              "10941  0.273270 -0.10251 -0.15400 -0.61960 -0.358480  0.97729  0.231860  \n",
              "10942  0.058908  0.51902  0.19820 -0.64066 -0.999100  0.48595 -0.586540  \n",
              "10943  0.157960  0.59973  0.01948  0.48689 -0.325070  0.36698 -0.179930  \n",
              "10944  0.211480 -0.11567 -0.18205 -0.87171 -0.028931  1.41150  0.280300  \n",
              "10945 -0.351660  0.41270 -0.39929  0.24495  0.651670  1.24470 -0.468070  \n",
              "\n",
              "[10946 rows x 100 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00f75b10-ccd3-4458-9a79-93c5952e0cca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.107670</td>\n",
              "      <td>0.11053</td>\n",
              "      <td>0.59812</td>\n",
              "      <td>-0.54361</td>\n",
              "      <td>0.673960</td>\n",
              "      <td>0.106630</td>\n",
              "      <td>0.038867</td>\n",
              "      <td>0.354810</td>\n",
              "      <td>0.063510</td>\n",
              "      <td>-0.094189</td>\n",
              "      <td>...</td>\n",
              "      <td>0.349510</td>\n",
              "      <td>-0.722600</td>\n",
              "      <td>0.375490</td>\n",
              "      <td>0.444100</td>\n",
              "      <td>-0.99059</td>\n",
              "      <td>0.61214</td>\n",
              "      <td>-0.35111</td>\n",
              "      <td>-0.831550</td>\n",
              "      <td>0.45293</td>\n",
              "      <td>0.082577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.038194</td>\n",
              "      <td>-0.24487</td>\n",
              "      <td>0.72812</td>\n",
              "      <td>-0.39961</td>\n",
              "      <td>0.083172</td>\n",
              "      <td>0.043953</td>\n",
              "      <td>-0.391410</td>\n",
              "      <td>0.334400</td>\n",
              "      <td>-0.575450</td>\n",
              "      <td>0.087459</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016215</td>\n",
              "      <td>-0.017099</td>\n",
              "      <td>-0.389840</td>\n",
              "      <td>0.874240</td>\n",
              "      <td>-0.72569</td>\n",
              "      <td>-0.51058</td>\n",
              "      <td>-0.52028</td>\n",
              "      <td>-0.145900</td>\n",
              "      <td>0.82780</td>\n",
              "      <td>0.270620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.339790</td>\n",
              "      <td>0.20941</td>\n",
              "      <td>0.46348</td>\n",
              "      <td>-0.64792</td>\n",
              "      <td>-0.383770</td>\n",
              "      <td>0.038034</td>\n",
              "      <td>0.171270</td>\n",
              "      <td>0.159780</td>\n",
              "      <td>0.466190</td>\n",
              "      <td>-0.019169</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063351</td>\n",
              "      <td>-0.674120</td>\n",
              "      <td>-0.068895</td>\n",
              "      <td>0.536040</td>\n",
              "      <td>-0.87773</td>\n",
              "      <td>0.31802</td>\n",
              "      <td>-0.39242</td>\n",
              "      <td>-0.233940</td>\n",
              "      <td>0.47298</td>\n",
              "      <td>-0.028803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.152900</td>\n",
              "      <td>-0.24279</td>\n",
              "      <td>0.89837</td>\n",
              "      <td>0.16996</td>\n",
              "      <td>0.535160</td>\n",
              "      <td>0.487840</td>\n",
              "      <td>-0.588260</td>\n",
              "      <td>-0.179820</td>\n",
              "      <td>-1.358100</td>\n",
              "      <td>0.425410</td>\n",
              "      <td>...</td>\n",
              "      <td>0.187120</td>\n",
              "      <td>-0.018488</td>\n",
              "      <td>-0.267570</td>\n",
              "      <td>0.727000</td>\n",
              "      <td>-0.59363</td>\n",
              "      <td>-0.34839</td>\n",
              "      <td>-0.56094</td>\n",
              "      <td>-0.591000</td>\n",
              "      <td>1.00390</td>\n",
              "      <td>0.206640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10941</th>\n",
              "      <td>-0.674900</td>\n",
              "      <td>1.24910</td>\n",
              "      <td>-0.20589</td>\n",
              "      <td>0.30409</td>\n",
              "      <td>0.956890</td>\n",
              "      <td>-0.070804</td>\n",
              "      <td>-0.739220</td>\n",
              "      <td>0.132180</td>\n",
              "      <td>0.625330</td>\n",
              "      <td>0.068480</td>\n",
              "      <td>...</td>\n",
              "      <td>0.267230</td>\n",
              "      <td>-0.344820</td>\n",
              "      <td>-0.247260</td>\n",
              "      <td>0.273270</td>\n",
              "      <td>-0.10251</td>\n",
              "      <td>-0.15400</td>\n",
              "      <td>-0.61960</td>\n",
              "      <td>-0.358480</td>\n",
              "      <td>0.97729</td>\n",
              "      <td>0.231860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10942</th>\n",
              "      <td>0.353960</td>\n",
              "      <td>-1.42890</td>\n",
              "      <td>0.74630</td>\n",
              "      <td>-0.20343</td>\n",
              "      <td>-0.580970</td>\n",
              "      <td>0.712080</td>\n",
              "      <td>-0.156970</td>\n",
              "      <td>0.330540</td>\n",
              "      <td>-0.120700</td>\n",
              "      <td>0.820530</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.162800</td>\n",
              "      <td>-0.129670</td>\n",
              "      <td>-0.301680</td>\n",
              "      <td>0.058908</td>\n",
              "      <td>0.51902</td>\n",
              "      <td>0.19820</td>\n",
              "      <td>-0.64066</td>\n",
              "      <td>-0.999100</td>\n",
              "      <td>0.48595</td>\n",
              "      <td>-0.586540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10943</th>\n",
              "      <td>0.168650</td>\n",
              "      <td>1.19110</td>\n",
              "      <td>0.17452</td>\n",
              "      <td>-0.27608</td>\n",
              "      <td>-0.265470</td>\n",
              "      <td>-0.307260</td>\n",
              "      <td>0.745120</td>\n",
              "      <td>0.485540</td>\n",
              "      <td>-0.078094</td>\n",
              "      <td>-0.079655</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.547290</td>\n",
              "      <td>0.382220</td>\n",
              "      <td>-0.207870</td>\n",
              "      <td>0.157960</td>\n",
              "      <td>0.59973</td>\n",
              "      <td>0.01948</td>\n",
              "      <td>0.48689</td>\n",
              "      <td>-0.325070</td>\n",
              "      <td>0.36698</td>\n",
              "      <td>-0.179930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10944</th>\n",
              "      <td>-0.324430</td>\n",
              "      <td>0.84650</td>\n",
              "      <td>-0.56920</td>\n",
              "      <td>-0.37743</td>\n",
              "      <td>1.048800</td>\n",
              "      <td>-1.067600</td>\n",
              "      <td>-0.334530</td>\n",
              "      <td>0.049853</td>\n",
              "      <td>0.732600</td>\n",
              "      <td>0.238410</td>\n",
              "      <td>...</td>\n",
              "      <td>0.086618</td>\n",
              "      <td>0.429520</td>\n",
              "      <td>-0.434800</td>\n",
              "      <td>0.211480</td>\n",
              "      <td>-0.11567</td>\n",
              "      <td>-0.18205</td>\n",
              "      <td>-0.87171</td>\n",
              "      <td>-0.028931</td>\n",
              "      <td>1.41150</td>\n",
              "      <td>0.280300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10945</th>\n",
              "      <td>0.309570</td>\n",
              "      <td>-0.30719</td>\n",
              "      <td>0.42018</td>\n",
              "      <td>-0.88324</td>\n",
              "      <td>0.138400</td>\n",
              "      <td>-0.130830</td>\n",
              "      <td>0.034690</td>\n",
              "      <td>1.586300</td>\n",
              "      <td>0.611050</td>\n",
              "      <td>0.484240</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000066</td>\n",
              "      <td>0.099137</td>\n",
              "      <td>0.531750</td>\n",
              "      <td>-0.351660</td>\n",
              "      <td>0.41270</td>\n",
              "      <td>-0.39929</td>\n",
              "      <td>0.24495</td>\n",
              "      <td>0.651670</td>\n",
              "      <td>1.24470</td>\n",
              "      <td>-0.468070</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10946 rows  100 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00f75b10-ccd3-4458-9a79-93c5952e0cca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-00f75b10-ccd3-4458-9a79-93c5952e0cca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-00f75b10-ccd3-4458-9a79-93c5952e0cca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "pd.DataFrame(embedding_matrix, index=list(train_test_val_vocabulary_i_w.values()).append('-PAD-'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the first embedding is a vector with only 0s, which we use to encode each zero of the pad."
      ],
      "metadata": {
        "id": "7dQWXAMsy186"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fjtBspZZzY5"
      },
      "source": [
        "Here, we can see the position of the embedding for 3 OOVs in the train ('telephone-information', 'vinken')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "id": "9_e-zNYMG6sJ",
        "outputId": "c414d42b-9c10-43e2-bb68-9405e908d861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Words are:  ['telephone-information', 'vinken']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x576 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzsAAAHSCAYAAADL+9VMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dbZhdZX02/PNKAkiIVjSRIiQM9uaAQIIBJhAENIhCEBRUKNKAIGAEFV+etoKdD6C3Y221WsMNUioo1iGAUZRWWxAI1QgIExohJFAiTcKbEApVMFJCuJ4PszMOkQTIy0xmze93HPuYvf7rWmv/9yw2yZm11rVLrTUAAABNM2ygGwAAANgUhB0AAKCRhB0AAKCRhB0AAKCRhB0AAKCRhB0AAKCRRgx0A+syevTo2tbWNtBtAAAAm7F58+Y9Vmsds2Z9sw47bW1t6e7uHug2AACAzVgpZekL1V3GBgAANJKwAwAANJKwAwAANJKwAwAANJKwAwAANJKwAwAANJKwAwAANJKwAwAANJKwAwAANJKwM4jceOONuemmmwa6DQAAGBSEnc3Ms88+u9Z16xN21rU/AABoMmFnA335y1/OhAkTMmHChPz93/99lixZkt122y3Tp0/P+PHjc8wxx2TFihVJknnz5uUtb3lL9tlnnxx22GF5+OGHkyRTp07NJz7xibS3t+erX/1q/vmf/zn77bdf9tprr7ztbW/LI488kiVLluTCCy/MV77ylUyaNCk//elPs2TJkrz1rW/NnnvumUMOOSTLli1Lkpx88sk5/fTTs99+++VTn/rUgP1uAABgII0Y6AYGs3nz5uUb3/hGfv7zn6fWmv322y9vectbcs899+Tiiy/OAQcckFNOOSUXXHBBPv7xj+fMM8/MD37wg4wZMyZXXHFFOjo6cskllyRJnnnmmXR3dydJnnjiidxyyy0ppeTrX/96/vZv/zZ/93d/l9NPPz2jRo3KX/zFXyRJ3vnOd+akk07KSSedlEsuuSQf+9jH8v3vfz9J8sADD+Smm27K8OHDB+aXAwAAA0zY2QBz587Nu9/97myzzTZJkve85z356U9/mrFjx+aAAw5IkpxwwgmZOXNmpk2blgULFuTtb397kmTVqlXZfvvte/d13HHH9T5/4IEHctxxx+Xhhx/OM888k5133vkFX//mm2/O9773vSTJiSee+LyzOMcee6ygAwDAkPail7GVUsaWUuaUUhaWUu4qpXy8VT+3lPJgKWV+6/GOPtt8upSyuJRyTynlsD71aa3a4lLK2ZvmLQ28UsofLNdas8cee2T+/PmZP39+7rzzzlx77bW9Y1YHpiQ588wz89GPfjR33nln/uEf/iFPP/30y+6h7/4AAGAoein37Dyb5M9rrbsnmZLkI6WU3VvrvlJrndR6/ChJWuvel2SPJNOSXFBKGV5KGZ7k/CSHJ9k9yfF99rNZm9XVlQltbRk+bFgmtLVlVldXkuSggw7K97///axYsSK//e1vc9VVV+Wggw7KsmXLcvPNNydJLrvsshx44IHZdddds3z58t76ypUrc9ddd73g6/3617/ODjvskCS59NJLe+uvfOUr8+STT/Yuv+lNb8rll1+eJOnq6spBBx208d88AAAMUi8admqtD9dab289fzLJoiQ7rGOTo5JcXmv931rrfyVZnGTf1mNxrfW+WuszSS5vjd2szerqSseMGTlv6dI8XWvOW7o0HTNmZFZXV/bee++cfPLJ2XfffbPffvvltNNOy7bbbptdd901559/fsaPH58nnngiZ5xxRrbccsvMnj07Z511Vt74xjdm0qRJa51Z7dxzz82xxx6bffbZJ6NHj+6tv/Od78xVV13VO0HBeeedl2984xvZc88980//9E/56le/2l+/FgAA2OyVWutLH1xKW5KfJJmQ5P9LcnKS3yTpTs/ZnydKKf8vyS211m+3trk4yb+2djGt1npaq35ikv1qrR9d4zVmJJmRJOPGjdtn6dKl6/veNooJbW05b+nSHNynNifJmTvtlAVLlvzB+CVLluTII4/MggUL+qtFAAAY0kop82qt7WvWX/LU06WUUUm+m+QTtdbfJPlakj9JMinJw0n+bmM0Wmu9qNbaXmttHzNmzMbY5QZZtGxZDlyjdmCrDgAAbL5eUtgppWyRnqDTVWv9XpLUWh+pta6qtT6X5B/Tc5lakjyYZGyfzXds1dZW36yNHzcuc9eozW3VX0hbW5uzOgAAsBl4KbOxlSQXJ1lUa/1yn/r2fYa9O8nqv+FfneR9pZStSik7J9klya1JbkuySyll51LKlumZxODqjfM2Np2Ozs6cOnJk5iRZmZ5L2E4dOTIdnZ0D3BkAALAuL+V7dg5IcmKSO0sp81u1v0rPbGqTktQkS5J8KElqrXeVUq5MsjA9M7l9pNa6KklKKR9Nck2S4UkuqbW+8HRkm5Hjp09PkpzZ0ZFFy5Zl/Lhx6ezs7K0DAACbp5c1QUF/a29vr93d3QPdBgAAsBnb4AkKAAAABhNhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhBwAAaCRhh+f5n//5n1xwwQUvOm7UqFHr/RptbW157LHH1nv7l6u7uzsf+9jHXnTczJkzM378+EyfPr0fuurxzW9+Mw899FDv8mmnnZaFCxf22+sDADRZqbUOdA9r1d7eXru7uwe6jSFlyZIlOfLII7NgwYJ1jhs1alSeeuqp9XqNtra2dHd3Z/To0eu1/aay22675brrrsuOO+74ksY/++yzGTFixAa95tSpU/OlL30p7e3tG7QfAIChrJQyr9b6B3+hcmaH5zn77LPzy1/+MpMmTcpf/uVf5otf/GImT56cPffcM+ecc84LbvNCY5YsWZLddtst06dPz/jx43PMMcdkxYoVvducd9552XvvvTNx4sTcfffdSZLHH388Rx99dPbcc89MmTIld9xxR5Lk3HPPzSmnnJKpU6fmDW94Q2bOnNm7n29/+9vZd999M2nSpHzoQx/KqlWr/qC/G2+8MUceeeQ693X66afnvvvuy+GHH56vfOUr6+zlxBNPzAEHHJATTzwx5557bk466aQcdNBB2WmnnfK9730vn/rUpzJx4sRMmzYtK1euTJJ89rOfzeTJkzNhwoTMmDEjtdbMnj073d3dmT59eiZNmpTf/e53mTp1alYH/FmzZmXixImZMGFCzjrrrN73M2rUqHR0dOSNb3xjpkyZkkceeWQ9jjQAQPMJOzzPF77whfzJn/xJ5s+fn7e//e259957c+utt2b+/PmZN29efvKTnzxv/LXXXrvWMffcc08+/OEPZ9GiRXnVq171vMvjRo8endtvvz1nnHFGvvSlLyVJzjnnnOy1116544478vnPfz7vf//7e8fffffdueaaa3LrrbfmM5/5TFauXJlFixbliiuuyM9+9rPMnz8/w4cPT1dX14u+xxfa14UXXpjXv/71mTNnTj75yU+us5eFCxfmuuuuy6xZs5Ikv/zlL3PDDTfk6quvzgknnJCDDz44d955Z7beeuv88Ic/TJJ89KMfzW233ZYFCxbkd7/7Xf7lX/4lxxxzTNrb29PV1ZX58+dn66237n2Nhx56KGeddVZuuOGGzJ8/P7fddlu+//3vJ0l++9vfZsqUKfnFL36RN7/5zfnHf/zHl3WMAQCGCmGHtbr22mtz7bXXZq+99sree++du+++O/fee+9LHjN27NgccMABSZITTjghc+fO7d3uPe95T5Jkn332yZIlS5Ikc+fOzYknnpgkeetb35r//u//zm9+85skyRFHHJGtttoqo0ePzute97o88sgjuf766zNv3rxMnjw5kyZNyvXXX5/77rvvRd/XC+1rTevq5V3vetfzgsnhhx+eLbbYIhMnTsyqVasybdq0JMnEiRN739ucOXOy3377ZeLEibnhhhty1113rbPH2267LVOnTs2YMWMyYsSITJ8+vTdEbrnllr1nqvr+/gAAeL4Nu+GAQW1WV1c6OzqyaNmyjB83Lh2dndm/FU6SpNaaT3/60/nQhz601n2sbcySJUtSSnlere/yVlttlSQZPnx4nn322RftdfX4vtvUWnPSSSflr//6r5839qqrrspnPvOZJMnXv/71l7Svl2ObbbZ5wf0NGzYsW2yxRe/7HDZsWJ599tk8/fTT+fCHP5zu7u6MHTs25557bp5++umX9Zp99X2N9ekfAGCocGZniJrV1ZWOGTNy3tKlebrWnLd0aTpmzMi1//ZvefLJJ5Mkhx12WC655JLeiQgefPDBPProo8/bz7rGLFu2LDfffHOS5LLLLsuBBx64zp4OOuig3svQbrzxxowePTqvetWr1jr+kEMOyezZs3tf7/HHH8/SpUvz7ne/O/Pnz8/8+fPX+8b/l9vLuqwONqNHj85TTz2V2bNn96575Stf2fv77mvffffNv//7v+exxx7LqlWrMmvWrLzlLW9Zr9cHABiqnNkZojo7OnLxihU5uLV8cJKLV6zImV/4Qg444IBMmDAhhx9+eP7sz/4s+++/f5KeG+O//e1v53Wve13vfg499NAsWrToD8YMHz48u+66a84///yccsop2X333XPGGWess6fVkwfsueeeGTlyZC699NJ1jt99993zuc99Loceemiee+65bLHFFjn//POz0047rffvZX17WZdXv/rV+eAHP5gJEybkj//4jzN58uTedSeffHJOP/30bL311r3BMEm23377fOELX8jBBx+cWmuOOOKIHHXUURv0ngAAhhpTTw9Rw4cNy9O1Zos+tZVJXlFKVj333Abv/6VOYQ0AABvK1NM8z/hx4zJ3jdrcVh0AAJpA2BmiOjo7c+rIkZmTnjM6c5KcOnJkOjo7N8r+29ranNUBAGBAuWdniDp++vQkyZl9ZmPr7OzsrQMAwGDnnh0AAGBQc88OAAAwpAg7AABAIwk7AABAIwk7AABAIwk7AABAIwk7AABAIwk7AABAIwk7AABAIwk7AABAIwk7AABAIwk7AABAIwk7AABAIwk7AABAIwk7AABAIwk7AABAI71o2CmljC2lzCmlLCyl3FVK+Xir/ppSyo9LKfe2fm7bqpdSysxSyuJSyh2llL377Ouk1vh7Syknbbq3BQAADHUv5czOs0n+vNa6e5IpST5SStk9ydlJrq+17pLk+tZykhyeZJfWY0aSryU94SjJOUn2S7JvknNWByQAAICN7UXDTq314Vrr7a3nTyZZlGSHJEclubQ17NIkR7eeH5XkW7XHLUleXUrZPslhSX5ca3281vpEkh8nmbZR3w0AAEDLy7pnp5TSlmSvJD9Psl2t9eHWql8l2a71fIck9/fZ7IFWbW11AACAje4lh51Syqgk303yiVrrb/quq7XWJHVjNFRKmVFK6S6ldC9fvnxj7BIAABiCXlLYKaVskZ6g01Vr/V6r/Ejr8rS0fj7aqj+YZGyfzXds1dZWf55a60W11vZaa/uYMWNeznsBAADo9VJmYytJLk6yqNb65T6rrk6yeka1k5L8oE/9/a1Z2aYk+XXrcrdrkhxaStm2NTHBoa0aAADARjfiJYw5IMmJSe4spcxv1f4qyReSXFlKOTXJ0iR/2lr3oyTvSLI4yYokH0iSWuvjpZT/m+S21rjP1lof3yjvAgAAYA2l53abzVN7e3vt7u4e6DYAAIDNWCllXq21fc36y5qNDQAAYLAQdgAAgEYSdgAAgEYSdgAAgEYSdgAAgEYSdgAAgEYSdgAAgEYSdgAAgEYSdgAAgEYSdgAAgEYSdgAAgEYSdgAAgEYSdgAAgEYSdgAAgEYSdgAAgEYSdhhSjj766Oyzzz7ZY489ctFFFw10OwAAbEIjBroB6E+XXHJJXvOa1+R3v/tdJk+enPe+97157WtfO9BtAQCwCQg7DCkzZ87MVVddlSS5//77c++99wo7AAAN5TI2GmdWV1cmtLVl+LBhmdDWllldXUmSG2+8Mdddd11uvvnm/OIXv8hee+2Vp59+eoC7BQBgU3Fmh0aZ1dWVjhkzcvGKFTkwydylS3PqjBlJkpGjRmXbbbfNyJEjc/fdd+eWW24Z2GYBANikhB0apbOjIxevWJGDW8sHJ7l4xYqc2dGReffckwsvvDDjx4/PrrvumilTpgxkqwAAbGKl1jrQPaxVe3t77e7uHug2GESGDxuWp2vNFn1qK5O8opSseu65gWoLAIBNqJQyr9bavmbdPTs0yvhx4zJ3jdrcVh0AgKFF2KFROjo7c+rIkZmTnjM6c5KcOnJkOjo7B7gzAAD6m3t2aJTjp09PkpzZ0ZFFy5Zl/Lhx6ezs7K0DADB0uGcHAAAY1NyzAwAADCnCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EjCDgAA0EgvGnZKKZeUUh4tpSzoUzu3lPJgKWV+6/GOPus+XUpZXEq5p5RyWJ/6tFZtcSnl7I3/VgAAAH7vpZzZ+WaSaS9Q/0qtdVLr8aMkKaXsnuR9SfZobXNBKWV4KWV4kvOTHJ5k9yTHt8YCAABsEiNebECt9SellLaXuL+jklxea/3fJP9VSlmcZN/WusW11vuSpJRyeWvswpfdMQAAwEuwIffsfLSUckfrMrdtW7UdktzfZ8wDrdra6gAAAJvE+oadryX5kySTkjyc5O82VkOllBmllO5SSvfy5cs31m4BAIAhZr3CTq31kVrrqlrrc0n+Mb+/VO3BJGP7DN2xVVtb/YX2fVGttb3W2j5mzJj1aQ8AAGD9wk4pZfs+i+9OsnqmtquTvK+UslUpZeckuyS5NcltSXYppexcStkyPZMYXL3+bQMAAKzbi05QUEqZlWRqktGllAeSnJNkaillUpKaZEmSDyVJrfWuUsqV6Zl44NkkH6m1rmrt56NJrkkyPMkltda7Nvq7AQAAaCm11oHuYa3a29trd3f3QLcBAABsxkop82qt7WvWN2Q2NgAAgM2WsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADSSsAMAADTSi4adUsolpZRHSykL+tReU0r5cSnl3tbPbVv1UkqZWUpZXEq5o5Syd59tTmqNv7eUctKmeTsAAAA9XsqZnW8mmbZG7ewk19dad0lyfWs5SQ5PskvrMSPJ15KecJTknCT7Jdk3yTmrAxIAAMCm8KJhp9b6kySPr1E+KsmlreeXJjm6T/1btcctSV5dStk+yWFJflxrfbzW+kSSH+cPAxQAAMBGs7737GxXa3249fxXSbZrPd8hyf19xj3Qqq2tDgAAsEls8AQFtdaapG6EXpIkpZQZpZTuUkr38uXLN9ZuAQCAIWZ9w84jrcvT0vr5aKv+YJKxfcbt2Kqtrf4Haq0X1Vrba63tY8aMWc/2AACAoW59w87VSVbPqHZSkh/0qb+/NSvblCS/bl3udk2SQ0sp27YmJji0VQMAANgkRrzYgFLKrCRTk4wupTyQnlnVvpDkylLKqUmWJvnT1vAfJXlHksVJViT5QJLUWh8vpfzfJLe1xn221rrmpAcAAAAbTem55Wbz1N7eXru7uwe6DQAAYDNWSplXa21fs77BExQAAABsjoQdAACgkYQdAACgkYQdAACgkYQdAACgkYQdAACgkYQdAACgkYQdAACgkYQdAACgkYQdAACgkYQdAACgkYQdAACgkYQdAACgkYQdAACgkYQdAAbEm970poFuAYCGE3YAGBA33XTTQLcAQMMJOwAMiFGjRiVJHn744bz5zW/OpEmTMmHChPz0pz8d4M4AaIoRA90AAEPbZZddlsMOOywdHR1ZtWpVVqxYMdAtAdAQwg4AA2ry5Mk55ZRTsnLlyhx99NGZNGnSQLcEQEO4jA2AAfXmN785P/nJT7LDDjvk5JNPzre+9a2BbgmAhhB2ANikurpmpa1tQoYNG562tgnp6pr1vPVLly7Ndtttlw9+8IM57bTTcvvttw9QpwA0jcvYANhkurpmZcaMjqxYcXGSA7N06dzMmHHq88bceOON+eIXv5gtttgio0aNcmYHgI2m1FoHuoe1am9vr93d3QPdBgDrqa1tQpYuPS/JwX2qc7LTTmdmyZIFA9UWAA1TSplXa21fs+4yNgA2mWXLFiU5cI3qga06AGxawg4Am8y4ceOTzF2jOrdVB4BNS9gBYJPp7OzIyJGnJpmTZGWSORk58tR0dnYMcGcADAUmKABgk5k+/fgkSUfHmVm2bFHGjRufzs7O3joAbEomKAAAAAY1ExQAAABDirADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA0krADAAA00gaFnVLKklLKnaWU+aWU7lbtNaWUH5dS7m393LZVL6WUmaWUxaWUO0ope2+MNwAAAPBCNsaZnYNrrZNqre2t5bOTXF9r3SXJ9a3lJDk8yS6tx4wkX9sIrw0AAPCCNsVlbEclubT1/NIkR/epf6v2uCXJq0sp22+C1wcAANjgsFOTXFtKmVdKmdGqbVdrfbj1/FdJtms93yHJ/X22faBVAwAA2OhGbOD2B9ZaHyylvC7Jj0spd/ddWWutpZT6cnbYCk0zkmTcuHEb2B4AADBUbdCZnVrrg62fjya5Ksm+SR5ZfXla6+ejreEPJhnbZ/MdW7U193lRrbW91to+ZsyYDWkPAAAYwtY77JRStimlvHL18ySHJlmQ5OokJ7WGnZTkB63nVyd5f2tWtilJft3ncjcAAICNakMuY9suyVWllNX7uazW+m+llNuSXFlKOTXJ0iR/2hr/oyTvSLI4yYokH9iA1wYAAFin9Q47tdb7krzxBer/neSQF6jXJB9Z39cDAAB4OTbF1NMAAAADTtgBAAAaSdgBAAAaSdgBAAD61bPPPtsvr7OhXyoKAAAMcUuWLMm0adMyZcqU3HTTTZk8eXI+8IEP5Jxzzsmjjz6arq6u/OhHP8ovf/nL3HfffRk3blxmzZq1yfsSdgAAgA22ePHifOc738kll1ySyZMn57LLLsvcuXNz9dVX5/Of/3wmTZqUhQsXZu7cudl66637pSeXsQEAABts5513zsSJEzNs2LDsscceOeSQQ1JKycSJE7NkyZIkybve9a5+CzqJsAMAAGwEW221Ve/zYcOG9S4PGzas9x6dbbbZpl97EnYAAICXpKtrVtraJmTYsOFpa5uQrq5Nf9/NhnDPDgAA8KK6umZlxoyOrFhxcZIDs3Tp3MyYcWqS5IAD9n/Z+5s0aVLmz5+/kbt8vlJr3aQvsCHa29trd3f3QLcBAABDXlvbhCxdel6Sg/tU52Snnc7MkiULBqqtJEkpZV6ttX3NusvYAACAF7Vs2aIkB65RPbBV3zwJOwAwCDz00EM55phjXnTcqFGj+qEbYCgaN258krlrVOe26psnYQcABoHXv/71mT179kC3AQxhnZ0dGTny1CRzkqxMMicjR56azs6OAe5s7YQdANjMnH322Tn//PN7l88999x86UtfyoQJE5Ik3/zmN/Oe97wn06ZNyy677JJPfepTf7CPxx57LPvvv39++MMfZvny5Xnve9+byZMnZ/LkyfnZz37Wu99TTjklU6dOzRve8IbMnDmzf94gMChNn358LrqoMzvtdGZKeUV22unMXHRRZ6ZPP36gW1srYQcANjPHHXdcrrzyyt7lK6+8Mvvtt9/zxsyfPz9XXHFF7rzzzlxxxRW5//77e9c98sgjOeKII/LZz342RxxxRD7+8Y/nk5/8ZG677bZ897vfzWmnndY79u67784111yTW2+9NZ/5zGeycuXKTf8GgUFr+vTjs2TJgjz33KosWbJgsw46iamnAWCzs9dee+XRRx/NQw89lOXLl2fbbbfN2LFjnzfmkEMOyR/90R8lSXbfffcsXbo0Y8eOzcqVK3PIIYfk/PPPz1ve8pYkyXXXXZeFCxf2bvub3/wmTz31VJLkiCOOyFZbbZWtttoqr3vd6/LII49kxx137Kd3CrBpCTsAsBk69thjM3v27K7TDGoAAAwpSURBVPzqV7/Kcccd9wfr+35T+fDhw3u/nXzEiBHZZ599cs011/SGneeeey633HJLXvGKV7zk/QA0gcvYAGCArOubyI877rhcfvnlmT17do499tiXvM9SSi655JLcfffd+Zu/+ZskyaGHHprzzjuvd8ym/hI/gM2FsAMAA2D1N5EvXXpean06S5eelxkzOnoDzx577JEnn3wyO+ywQ7bffvuXte/hw4dn1qxZueGGG3LBBRdk5syZ6e7uzp577pndd989F1544aZ4SwCbnVJrHege1qq9vb12d3cPdBsAsNFtzt9EDjDYlFLm1Vrb16w7swMAA2AwfhM5wGAj7ADAABiM30QOMNgIOwAwAAbjN5EDDDbCDtCv1jX71CGHHJIHH3xwALuD/jMYv4kcYLDxPTtAv1k9+9SKFRcnOTBLl87NjBmnJkmOP/64LF68OK95zWsGtknoR9OnHy/cAGxCwg7Qbzo6OltBZ/XsUwdnxYqL09FxZt74xol573vfm6233nogWwQAGsTU00C/GTZseGp9OskWfaorU8or8txzqwaqLQBgkDP1NDDgzD4FAPQnYQfoN2afAgD6k3t2gH6z+kbsjo4zs2zZoowbNz6dnWafAgA2DffsAAAAg5p7dgAAgCFF2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABpJ2AEAABqp38NOKWVaKeWeUsriUsrZ/f36AADA0NCvYaeUMjzJ+UkOT7J7kuNLKbv3Zw8AAMDQ0N9ndvZNsrjWel+t9Zkklyc5qp97AAAAhoD+Djs7JLm/z/IDrVqvUsqMUkp3KaV7+fLl/docAADQHJvdBAW11otqre211vYxY8YMdDsAAMAg1d9h58EkY/ss79iqAQAAbFT9HXZuS7JLKWXnUsqWSd6X5Op+7gEAABgCRvTni9Vany2lfDTJNUmGJ7mk1npXf/YAAAAMDf0adpKk1vqjJD/q79cFAACGls1uggIAAICNQdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBAAAaSdgBADZrp512WhYuXDjQbQCD0IiBbgAAYF2+/vWvv6zxq1atyvDhw9e6DAwdzuwAAJuFJUuWZLfddsv06dMzfvz4HHPMMVmxYkWmTp2a7u7uJMm1116b/fffP3vvvXeOPfbYPPXUU0mStra2nHXWWdl7773zne985w+W17Yd0GzCDgCw2bjnnnvy4Q9/OIsWLcqrXvWqXHDBBb3rHnvssXzuc5/Lddddl9tvvz3t7e358pe/3Lv+ta99bW6//fa8733ve97y2972tnVuBzSXy9gAgM3G2LFjc8ABByRJTjjhhMycObN33S233JKFCxf2rn/mmWey//77964/7rjjnrev1csvth3QXMIOANCvurpmpaOjM8uWLcq4cePT2dmR6dOPT5KUUp43tu9yrTVvf/vbM2vWrBfc7zbbbPOCyy+2HdBcLmMDAPpNV9eszJjRkaVLz0utT2fp0vMyY0ZHurp6gsiyZcty8803J0kuu+yyHHjggb3bTpkyJT/72c+yePHiJMlvf/vb/Od//ueLvub6bgcMfsIOANBvOjo6s2LFxUkOTrJFkoOzYsXF6ejoTJLsuuuuOf/88zN+/Pg88cQTOeOMM3q3HTNmTL75zW/m+OOPz5577pn9998/d99994u+5vpuBwx+pdY60D2sVXt7e109+woAMPgNGzY8tT6dnqCz2sqU8orcd98vc+SRR2bBggUD1R4wSJVS5tVa29esO7MDAPSbcePGJ5m7RnVuqw6wcQk7AEC/6ezsyMiRpyaZk2RlkjkZOfLUdHZ2pK2tzVkdYKMyGxsA0G9Wz7rW0XFmn9nYOnvrABuTe3YAAIBBzT07AADAkCLsAAAAjSTsAAAAjSTsAAAAjSTsAAAAjSTsAAAAjSTsAAAAjSTsAAAAjSTsAAAAjSTsAAAAjSTsAAAAjSTsAAAAjSTsAAAAjSTsAAAAjSTsAAAAjVRqrQPdw1qVUpYnWTrQffSz0UkeG+gm6BeO9dDhWA8NjvPQ4VgPHY714LFTrXXMmsXNOuwMRaWU7lpr+0D3wabnWA8djvXQ4DgPHY710OFYD34uYwMAABpJ2AEAABpJ2Nn8XDTQDdBvHOuhw7EeGhznocOxHjoc60HOPTsAAEAjObMDAAA0krAzQEopXyyl3F1KuaOUclUp5dV91n26lLK4lHJPKeWwPvVprdriUsrZA9M5G8pxbJZSythSypxSysJSyl2llI+36q8ppfy4lHJv6+e2rXoppcxsHf87Sil7D+w74OUopQwvpfxHKeVfWss7l1J+3jqeV5RStmzVt2otL26tbxvIvnn5SimvLqXMbv1ZvaiUsr/PdfOUUj7Z+n/3glLKrFLKK3yum0XYGTg/TjKh1rpnkv9M8ukkKaXsnuR9SfZIMi3JBa0/XIcnOT/J4Ul2T3J8ayyDiOPYSM8m+fNa6+5JpiT5SOuYnp3k+lrrLkmuby0nPcd+l9ZjRpKv9X/LbICPJ1nUZ/lvknyl1vp/kjyR5NRW/dQkT7TqX2mNY3D5apJ/q7XuluSN6TnuPtcNUkrZIcnHkrTXWickGZ6ev4P5XDeIsDNAaq3X1lqfbS3ekmTH1vOjklxea/3fWut/JVmcZN/WY3Gt9b5a6zNJLm+NZXBxHBum1vpwrfX21vMn0/MXoh3Sc1wvbQ27NMnRredHJflW7XFLkleXUrbv57ZZD6WUHZMckeTrreWS5K1JZreGrHmcVx//2UkOaY1nECil/FGSNye5OElqrc/UWv8nPtdNNCLJ1qWUEUlGJnk4PteNIuxsHk5J8q+t5zskub/PugdatbXVGVwcxwZrXdKwV5KfJ9mu1vpwa9WvkmzXeu6/gcHr75N8KslzreXXJvmfPv9w1fdY9h7n1vpft8YzOOycZHmSb7QuW/x6KWWb+Fw3Sq31wSRfSrIsPSHn10nmxee6UYSdTaiUcl3rGtA1H0f1GdORnstgugauU2BDlVJGJflukk/UWn/Td13tmfbS1JeDWCnlyCSP1lrnDXQv9IsRSfZO8rVa615JfpvfX7KWxOe6CVr3XB2VnnD7+iTbpOcWAhpkxEA30GS11reta30p5eQkRyY5pP5+DvAHk4ztM2zHVi3rqDN4rOv4MkiVUrZIT9DpqrV+r1V+pJSyfa314dblLI+26v4bGJwOSPKuUso7krwiyavSc0/Hq0spI1r/ytv3WK4+zg+0Lo/5oyT/3f9ts54eSPJArfXnreXZ6Qk7PtfN8rYk/1VrXZ4kpZTvpeez7nPdIM7sDJBSyrT0XA7xrlrrij6rrk7yvtaMHzun52bHW5PclmSX1gwhW6bnBrqr+7tvNpjj2DCt67UvTrKo1vrlPquuTnJS6/lJSX7Qp/7+1uxNU5L8us9lMWymaq2frrXuWGttS8/n9oZa6/Qkc5Ic0xq25nFeffyPaY13FmCQqLX+Ksn9pZRdW6VDkiyMz3XTLEsypZQysvX/8tXH2ee6QXyp6AAppSxOslV+/y8Ct9RaT2+t60jPfTzPpueSmH9t1d+RnmvGhye5pNba2e+Ns8Ecx2YppRyY5KdJ7szv7+X4q/Tct3NlknFJlib501rr460/UP9fei6VWJHkA7XW7n5vnPVWSpma5C9qrUeWUt6QnolGXpPkP5KcUGv931LKK5L8U3ru4Xo8yftqrfcNVM+8fKWUSemZjGLLJPcl+UB6/pHY57pBSimfSXJcev7O9R9JTkvPvTk+1w0h7AAAAI3kMjYAAKCRhB0AAKCRhB0AAKCRhB0AAKCRhB0AAKCRhB0AAKCRhB0AAKCRhB0AAKCR/n/BoaMSl6nsKgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "strings = ['telephone-information', 'vinken']\n",
        "contexts = []\n",
        "sep = []\n",
        "for string in strings:\n",
        "  index = train_vocabulary_w_i[string]\n",
        "  current = [train_vocabulary_i_w[c] for c in np.where(co_occurrence_train[index] != 0)[0]]\n",
        "  contexts = contexts.copy() + current.copy()\n",
        "  contexts.append(string)\n",
        "  sep.append(len(current))\n",
        "\n",
        "embeddings = [train_val_test_glove_vocabulary[context_word] for context_word in contexts]\n",
        "\n",
        "word_vectors = embeddings\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=1, n_iter=10000, perplexity=2)\n",
        "np.set_printoptions(suppress=True)\n",
        "T = tsne.fit_transform(word_vectors)\n",
        "labels = contexts\n",
        "\n",
        "prev = 0\n",
        "color = ['red', 'blue', 'yellow']\n",
        "count = 0\n",
        "for i in sep:\n",
        "  plt.scatter(T[prev:prev+i+1, 0], T[prev:prev+i+1, 1], c=color[count], edgecolors='k')\n",
        "  prev = prev+i+1\n",
        "  count += 1\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "    plt.annotate(label, xy=(x+5, y+5), xytext=(0, 0), textcoords='offset points')\n",
        "print()\n",
        "print(\"Words are: \", strings)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we do the same for with the random embeddings."
      ],
      "metadata": {
        "id": "TvJa5uKrDXOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_embedding_matrix = build_embedding_matrix(random_vocabulary, train_test_val_vocabulary_w_i, columns=dimension)\n",
        "random_embedding_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtTVkmz0DIbp",
        "outputId": "b5602a5a-71dd-46fe-d49d-2a7c218c00e2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10946, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQQpDqgI4bE-"
      },
      "source": [
        "## 8. Padding and Truncation for X_train, X_val, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAwGVzURDRUU"
      },
      "source": [
        "Here, we encode the sentences using the Tokenizer() and then we apply zero-padding for the sentences with a length less than the 75 quantile, while we truncate sentences with lentgh grater than the 75 percentile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF3uZtCnhfhK",
        "outputId": "6cd52538-9f4f-4ed3-cb0a-8f447046e0f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \t\t pierre vinken , 61 years old , will join the board as a nonexecutive director nov. 29 .\n",
            "Encode for that row:     [5419, 3714, 1, 2005, 78, 316, 1, 39, 2383, 2, 122, 22, 6, 2006, 317, 444, 2007, 3]\n",
            "Padding for that row:    [5419, 3714, 1, 2005, 78, 316, 1, 39, 2383, 2, 122, 22, 6, 2006, 317, 444, 2007, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "Shape for X_train:  (1936, 31)\n",
            "Shape for X_val:    (1256, 31)\n",
            "Shape for X_test:   (636, 31)\n"
          ]
        }
      ],
      "source": [
        "# TEXT2SEQUENCE --> PADDING\n",
        "\n",
        "# as before, we can use a single tokenizer (stored in a file), but the split are \n",
        "# always equal and we wanted a notebook easy to unserstans, step by step\n",
        "word_tokenizer = tf.keras.preprocessing.text.Tokenizer(split=' ', num_words=0, filters='\\t\\n')\n",
        "word_tokenizer.fit_on_texts(df.Words)\n",
        "\n",
        "\n",
        "# ---------TRAIN---------\n",
        "# encode of the sentences\n",
        "X_train = word_tokenizer.texts_to_sequences(train.Words)\n",
        "print('Sentence: \\t\\t', train.Words.iloc[0])\n",
        "print('Encode for that row:    ', X_train[0])\n",
        "# apply the padding\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding = 'post', maxlen = max_length, truncating='post')\n",
        "print('Padding for that row:   ', [k for k in X_train[0]])\n",
        "print()\n",
        "\n",
        "\n",
        "# ---------VALIDATION---------\n",
        "X_val = word_tokenizer.texts_to_sequences(validation.Words)\n",
        "X_val = tf.keras.preprocessing.sequence.pad_sequences(X_val, padding = 'post', maxlen = max_length, truncating='post')\n",
        "\n",
        "\n",
        "# ---------TEST---------\n",
        "X_test = word_tokenizer.texts_to_sequences(test.Words)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding = 'post', maxlen = max_length, truncating='post')\n",
        "\n",
        "print('Shape for X_train: ', X_train.shape)\n",
        "print('Shape for X_val:   ', X_val.shape)\n",
        "print('Shape for X_test:  ', X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N0ZIFLez182"
      },
      "source": [
        "## 9. Tags: y_train, y_val, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nN6iWCCELAb"
      },
      "source": [
        "Then, we do the same previous process for the labels y_train, y_val, y_test. Since the output of the Dense Layer is in the one-hot encoder form, we need to transform the tags. To do this we use the keras function to_categorical, which transform each tag in a one-hot encoder vector. We encoded also the PAD tag: since the Keras Dense Layer doesn't support masking, our models need also to learn the mapping of zeros of the padding to their class 'PAD'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YNT18PI0PyA",
        "outputId": "4905996f-facf-44f3-f468-95acff7427a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The unique tags are: 45, plus -PAD-\n",
            "Tags are:  {'nn': 1, 'in': 2, 'nnp': 3, 'dt': 4, 'nns': 5, 'jj': 6, ',': 7, '.': 8, 'cd': 9, 'vbd': 10, 'rb': 11, 'vb': 12, 'cc': 13, 'to': 14, 'vbn': 15, 'vbz': 16, 'prp': 17, 'vbg': 18, 'vbp': 19, 'md': 20, 'pos': 21, 'prp$': 22, '$': 23, '``': 24, \"''\": 25, ':': 26, 'wdt': 27, 'jjr': 28, 'nnps': 29, 'wp': 30, 'rp': 31, 'jjs': 32, 'wrb': 33, 'rbr': 34, ')': 35, '(': 36, 'ex': 37, 'rbs': 38, 'pdt': 39, '#': 40, 'wp$': 41, 'ls': 42, 'fw': 43, 'uh': 44, 'sym': 45}\n",
            "\n",
            "Sentence: \t\t\t pierre vinken , 61 years old , will join the board as a nonexecutive director nov. 29 .\n",
            "The tags for that row are: \t nnp nnp , cd nns jj , md vb dt nn in dt jj nn nnp cd .\n",
            "The encode for the tags are: \t [3, 3, 7, 9, 5, 6, 7, 20, 12, 4, 1, 2, 4, 6, 1, 3, 9, 8]\n",
            "The inverse of the sequence is:  NNP NNP , CD NNS JJ , MD VB DT NN IN DT JJ NN NNP CD .\n",
            "The encode + pad for that row are:  [ 3  3  7  9  5  6  7 20 12  4  1  2  4  6  1  3  9  8  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n",
            "The inverse of the pad sequence is:  nnp nnp , cd nns jj , md vb dt nn in dt jj nn nnp cd .\n",
            "Shape for that row:  (31, 46)\n",
            "(31, 46)\n",
            "\n",
            "\n",
            "Shape for y_train:  (1936, 31, 46)\n",
            "Shape for y_val:    (1256, 31, 46)\n",
            "Shape for y_test:   (636, 31, 46)\n"
          ]
        }
      ],
      "source": [
        "# as before, we haven't use functions for an understandable notebook\n",
        "\n",
        "# find the unique tags in the whole document using Tokenizer()\n",
        "tag_tokenizer = tf.keras.preprocessing.text.Tokenizer(split=' ', num_words=0, filters='\\t\\n')\n",
        "tag_tokenizer.fit_on_texts(df.Tags)\n",
        "n_tags = len(tag_tokenizer.word_index)+1 # the tokenizer starts from 1 for the encoding (0 reserved to the -PAD-)\n",
        "print('The unique tags are: {0}, plus -PAD-'.format(n_tags-1))\n",
        "print('Tags are: ', tag_tokenizer.word_index)\n",
        "print()\n",
        "\n",
        "\n",
        "# ---------TRAIN---------\n",
        "\n",
        "# encode the train tags using the previous Tokenizer()\n",
        "y_train = tag_tokenizer.texts_to_sequences(train.Tags)\n",
        "print('Sentence: \\t\\t\\t', train.Words.iloc[0])\n",
        "print('The tags for that row are: \\t', train.Tags.iloc[0])\n",
        "print('The encode for the tags are: \\t', y_train[0])\n",
        "print('The inverse of the sequence is: ', tag_tokenizer.sequences_to_texts(y_train)[0].upper())\n",
        "\n",
        "# add padding\n",
        "y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train, padding = 'post', maxlen = max_length, truncating='post')\n",
        "print('The encode + pad for that row are: ', y_train[0])\n",
        "print('The inverse of the pad sequence is: ', tag_tokenizer.sequences_to_texts(y_train)[0])\n",
        "\n",
        "# one hot encoder for each tag in the sequence\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes = n_tags)\n",
        "print('Shape for that row: ', y_train[0].shape)\n",
        "print(y_train[0].shape)\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n",
        "# ---------VALIDATION---------\n",
        "y_val = tag_tokenizer.texts_to_sequences(validation.Tags)\n",
        "y_val = tf.keras.preprocessing.sequence.pad_sequences(y_val, padding = 'post', maxlen = max_length, truncating='post')\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes = n_tags)\n",
        "\n",
        "\n",
        "# ---------TEST---------\n",
        "y_test = tag_tokenizer.texts_to_sequences(test.Tags)\n",
        "y_test = tf.keras.preprocessing.sequence.pad_sequences(y_test, padding = 'post', maxlen = max_length, truncating='post')\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes = n_tags)\n",
        "\n",
        "print('Shape for y_train: ', y_train.shape)\n",
        "print('Shape for y_val:   ', y_val.shape)\n",
        "print('Shape for y_test:  ', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFx5EJGU4BWo"
      },
      "source": [
        "## 10. The models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urb-5hFjsxV-"
      },
      "source": [
        "For this task we used the four models suggested in the instructions:\n",
        "- the BiLSTM model, which has a bidirectional LSTM followed by a Dense Layer;\n",
        "- the BiGRU model, which has a bidierectional GRU followed by a Dense Layer;\n",
        "- BiDense model, where two Dense Layers follow a bidirectional LSTM\n",
        "- ML-BiLSTM, which uses two Bidirectional LSTM followed by a Dense Layer.<br>\n",
        "\n",
        "Each Dense Layer used is actually a Time Distributed Dense layer, which is used as 'classification head' with a softmax activation function (except for the Bi_Dense model, which uses a Time Distributed Layer as hidden layer). \n",
        "\n",
        "For each architecture, after the input layer there is the embedding layer that replaces each word: in particular, each zero of padding is replaced by a vector of only zeros taken from the embedding matrix. As we said before, Keras Dense Layer doesn't support masking, so each model has also to learn the mapping from 0 to the class '-PAD-'.\n",
        "\n",
        "The tuning of the hyperparameters was performed on the validation set. For each model, we used a fixed batch size equal to 32 and n. of epochs equal to 30. To avoid overfitting, we used dropout to the input conmnections within the LSTM/GRU layer and also the L2 recurrent regularization.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n_epochs = 30\n",
        "verbose = 0\n",
        "\n",
        "y_pred_val = {}\n",
        "histories = {}\n",
        "models = {}\n",
        "\n",
        "set_reproducibility(random_seed)"
      ],
      "metadata": {
        "id": "7JezEJcg78cp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BiLSTM Model**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The first model is a Bidirectional LSTM followed by a Time Distributed Layer that acts as 'classification head'. We found that the best number of units for the Bidirectional LSTM layer is 64, and we also notice that by using a number of units greater than 100 the performance on the validation set doesn't improve. Here the input dropout value is 0.2.\n"
      ],
      "metadata": {
        "id": "_lgBa-uNiLnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape=max_length, name = \"Input_Layer\")\n",
        "embed_layer = Embedding(input_dim=embedding_matrix.shape[0], output_dim=dimension, weights=[embedding_matrix], input_length=max_length, trainable=False, name = 'Embedding_Layer')(input)\n",
        "bi_LSTM = Bidirectional(LSTM(64, return_sequences=True, dropout = 0.2, recurrent_regularizer = 'l2', name = 'LSTM', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=random_seed), bias_initializer=tf.keras.initializers.zeros, recurrent_initializer=tf.keras.initializers.zeros), name='BiLSTM_Layer')(embed_layer)\n",
        "dense_layer = TimeDistributed(Dense(n_tags, activation='softmax', name = 'DenseLayer', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=random_seed), bias_initializer=tf.keras.initializers.zeros), name='TimeDistributed_Layer')(bi_LSTM)\n",
        "model_Bi_LSTM = Model(input, dense_layer, name='BiLSTM')\n",
        "models['BiLSTM']=model_Bi_LSTM\n",
        "model_Bi_LSTM.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yihvoLKxaR9p",
        "outputId": "25ee29d4-2b9e-4912-8094-c1da5fe6c2fb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"BiLSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Input_Layer (InputLayer)    [(None, 31)]              0         \n",
            "                                                                 \n",
            " Embedding_Layer (Embedding)  (None, 31, 100)          1094600   \n",
            "                                                                 \n",
            " BiLSTM_Layer (Bidirectional  (None, 31, 128)          84480     \n",
            " )                                                               \n",
            "                                                                 \n",
            " TimeDistributed_Layer (Time  (None, 31, 46)           5934      \n",
            " Distributed)                                                    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,185,014\n",
            "Trainable params: 90,414\n",
            "Non-trainable params: 1,094,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we build the same model using the random embeddings. "
      ],
      "metadata": {
        "id": "Gwk6AJH6DyCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape=max_length, name = \"Input_Layer\")\n",
        "embed_layer = Embedding(input_dim=embedding_matrix.shape[0], output_dim=dimension, weights=[random_embedding_matrix], input_length=max_length, trainable=False, name = 'Embedding_Layer')(input)\n",
        "bi_LSTM = Bidirectional(LSTM(64, return_sequences=True, dropout = 0.2, recurrent_regularizer = 'l2', name = 'LSTM', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=random_seed), bias_initializer=tf.keras.initializers.zeros, recurrent_initializer=tf.keras.initializers.zeros), name='BiLSTM_Layer')(embed_layer)\n",
        "dense_layer = TimeDistributed(Dense(n_tags, activation='softmax', name = 'DenseLayer', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=random_seed), bias_initializer=tf.keras.initializers.zeros), name='TimeDistributed_Layer')(bi_LSTM)\n",
        "model_random_Bi_LSTM = Model(input, dense_layer, name='Random_BiLSTM')\n",
        "models['Random_BiLSTM']=model_random_Bi_LSTM\n",
        "model_random_Bi_LSTM.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5bw8EsGDqL4",
        "outputId": "cb5b290d-ffff-45b5-a5de-da627bc31c58"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Random_BiLSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Input_Layer (InputLayer)    [(None, 31)]              0         \n",
            "                                                                 \n",
            " Embedding_Layer (Embedding)  (None, 31, 100)          1094600   \n",
            "                                                                 \n",
            " BiLSTM_Layer (Bidirectional  (None, 31, 128)          84480     \n",
            " )                                                               \n",
            "                                                                 \n",
            " TimeDistributed_Layer (Time  (None, 31, 46)           5934      \n",
            " Distributed)                                                    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,185,014\n",
            "Trainable params: 90,414\n",
            "Non-trainable params: 1,094,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BiGRU Model**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The second model is a Bidirectional GRU followed by a Time Distributed Layer that acts as 'classification head'. We found that the best number of units for the Bidirectional GRU layer is 64 (the same of the BiLSTM model), and we also notice that, like in the previous model,  by using a number of units greater than 100 the performance on the validation set doesn't improve. Here the input dropout value is 0.1."
      ],
      "metadata": {
        "id": "yapFDzGNmVsC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iErStepWNnLI",
        "outputId": "d077233d-effd-4cd8-d2d3-7d42951212f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"BiGRU\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Input_Layer (InputLayer)    [(None, 31)]              0         \n",
            "                                                                 \n",
            " Embedding_Layer (Embedding)  (None, 31, 100)          1094600   \n",
            "                                                                 \n",
            " BiGRU_Layer (Bidirectional)  (None, 31, 128)          63744     \n",
            "                                                                 \n",
            " TimeDistributed_Layer (Time  (None, 31, 46)           5934      \n",
            " Distributed)                                                    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,164,278\n",
            "Trainable params: 69,678\n",
            "Non-trainable params: 1,094,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input = Input(shape=max_length, name = \"Input_Layer\")\n",
        "embed_layer = Embedding(input_dim=embedding_matrix.shape[0], name = 'Embedding_Layer', output_dim=dimension, weights=[embedding_matrix], input_length=max_length, trainable=False)(input)\n",
        "bi_LSTM = Bidirectional(GRU(64, return_sequences=True, dropout = 0.1, recurrent_regularizer = 'l2', name = 'GRU'), name='BiGRU_Layer')(embed_layer)\n",
        "dense_layer = TimeDistributed(Dense(n_tags, activation='softmax', name = 'DenseLayer'), name='TimeDistributed_Layer')(bi_LSTM)\n",
        "model_Bi_GRU = Model(input, dense_layer, name='BiGRU')\n",
        "models['BiGRU']=model_Bi_GRU\n",
        "model_Bi_GRU.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BiDense Model**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The third model is a Bidirectional LSTM followed by two Time Distributed Layers.\n",
        "\n",
        "Here we havea Bidirectional LSTM layer with 30 units, which is followed by two Time Distributed Dense Layers. The last one is the 'classification head', while the penultimate one is identical to the latter but uses ReLu as activation function.  Here the input dropout value for the Bidirectional LSTM is 0.1."
      ],
      "metadata": {
        "id": "XtOyMtiUnCxz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaAiaOKwNw77",
        "outputId": "459bb9da-33d6-44d8-97d8-217772ae7963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"BiDense\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Input_Layer (InputLayer)    [(None, 31)]              0         \n",
            "                                                                 \n",
            " Embedding_Layer (Embedding)  (None, 31, 100)          1094600   \n",
            "                                                                 \n",
            " BiLSTM_Layer (Bidirectional  (None, 31, 60)           31440     \n",
            " )                                                               \n",
            "                                                                 \n",
            " TimeDistributed1_Layer (Tim  (None, 31, 46)           2806      \n",
            " eDistributed)                                                   \n",
            "                                                                 \n",
            " TimeDistributed2_Layer (Tim  (None, 31, 46)           2162      \n",
            " eDistributed)                                                   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,131,008\n",
            "Trainable params: 36,408\n",
            "Non-trainable params: 1,094,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input = Input(shape=max_length, name = \"Input_Layer\")\n",
        "embed_layer = Embedding(input_dim=embedding_matrix.shape[0], name = 'Embedding_Layer', output_dim=dimension, weights=[embedding_matrix], input_length=max_length, trainable=False)(input)\n",
        "bi_LSTM = Bidirectional(LSTM(30, return_sequences=True, dropout = 0.1, recurrent_regularizer = 'l2'), name='BiLSTM_Layer')(embed_layer) # When return_sequences is set to False, Dense is applied to the last time step only\n",
        "dense_layer = TimeDistributed(Dense(n_tags, activation='ReLU'), name='TimeDistributed1_Layer')(bi_LSTM)\n",
        "dense_layer = TimeDistributed(Dense(n_tags, activation='softmax'), name='TimeDistributed2_Layer')(dense_layer)\n",
        "model_Bi_Dense = Model(input, dense_layer, name='BiDense')\n",
        "models['BiDense']=model_Bi_Dense\n",
        "model_Bi_Dense.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML_BiLSTM Model**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The last Model uses two Bidirectional LSTM layers, with 64 and 32 units respectively, followed by the usual 'classification head'. It's interesting to see that here we applied input dropout (with value equal to 0.1) to only the first Bi_LSTM layer:\n"
      ],
      "metadata": {
        "id": "sG0fHPD7Kpyn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sAVharpFaCh",
        "outputId": "5b997084-261f-4022-f59c-779b0eebff86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"ML_BiLSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Input_Layer (InputLayer)    [(None, 31)]              0         \n",
            "                                                                 \n",
            " Embedding_Layer (Embedding)  (None, 31, 100)          1094600   \n",
            "                                                                 \n",
            " BiLSTM1_Layer (Bidirectiona  (None, 31, 128)          84480     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " BiLSTM2_Layer (Bidirectiona  (None, 31, 64)           41216     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " TimeDistributed_Layer (Time  (None, 31, 46)           2990      \n",
            " Distributed)                                                    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,223,286\n",
            "Trainable params: 128,686\n",
            "Non-trainable params: 1,094,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input = Input(shape=max_length, name = \"Input_Layer\")\n",
        "embed_layer = Embedding(input_dim=embedding_matrix.shape[0], name = 'Embedding_Layer', output_dim=dimension, weights=[embedding_matrix], input_length=max_length, trainable=False)(input)\n",
        "bi_LSTM = Bidirectional(LSTM(64, return_sequences=True, dropout = 0.1, recurrent_regularizer = 'l2'), name='BiLSTM1_Layer')(embed_layer) # When return_sequences is set to False, Dense is applied to the last time step only\n",
        "bi_LSTM = Bidirectional(LSTM(32, return_sequences=True, recurrent_regularizer = 'l2'), name='BiLSTM2_Layer')(bi_LSTM) # When return_sequences is set to False, Dense is applied to the last time step only\n",
        "dense_layer = TimeDistributed(Dense(n_tags, activation='softmax'), name='TimeDistributed_Layer')(bi_LSTM)\n",
        "model_Double_BiLSTM = Model(input, dense_layer, name='ML_BiLSTM')\n",
        "models['ML_BiLSTM']=model_Double_BiLSTM\n",
        "model_Double_BiLSTM.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we trained each model with a decreasing learning rate (0.1 --> 0.01 --> 0.001) over the epochs. "
      ],
      "metadata": {
        "id": "rPyPxj-Bn3YH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO1-tsfBj7Mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1d5a593-882a-4689-9b6d-4b59cbcb5e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************* Training for: BiLSTM *************************\n",
            "40/40 [==============================] - 1s 3ms/step\n",
            "\n",
            "\n",
            "************************* Training for: Random_BiLSTM *************************\n",
            "40/40 [==============================] - 1s 3ms/step\n",
            "\n",
            "\n",
            "************************* Training for: BiGRU *************************\n",
            "40/40 [==============================] - 1s 3ms/step\n",
            "\n",
            "\n",
            "************************* Training for: BiDense *************************\n",
            "40/40 [==============================] - 1s 3ms/step\n",
            "\n",
            "\n",
            "************************* Training for: ML_BiLSTM *************************\n",
            "40/40 [==============================] - 1s 5ms/step\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "checkpoint_path = \"training_weights\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "\n",
        "    if epoch < n_epochs*0.2:\n",
        "        return 0.1\n",
        "   \n",
        "    elif epoch < n_epochs*0.8 and epoch >= n_epochs*0.2:\n",
        "        return 0.01\n",
        "    \n",
        "    else:\n",
        "        return 0.001\n",
        "\n",
        "cbk = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "\n",
        "for i in list(models.keys()):\n",
        "  print('************************* Training for:', models[i].name, '*************************')\n",
        "  #mcp_save = ModelCheckpoint(checkpoint_path+'/{0}_{1}.hdf5'.format(models[i].name, random_seed), save_best_only=True, monitor='val_acc', mode='min')\n",
        "  adam = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
        "  models[i].compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
        "  histories[i]=models[i].fit(X_train, y_train, epochs = n_epochs, batch_size = batch_size, callbacks = [callback, cbk], validation_data = (X_val, y_val), verbose = verbose)\n",
        "  print('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can see the accuracy of each model on both training and validation set over the epochs."
      ],
      "metadata": {
        "id": "NLvv-MleolvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize = (23,5))\n",
        "\n",
        "count = 0\n",
        "for j in ['BiLSTM', 'BiGRU', 'BiDense', 'ML_BiLSTM']: \n",
        "  plt.subplot(1, 4, count+1)\n",
        "  count += 1\n",
        "  plt.plot(histories[j].history['acc'])\n",
        "  plt.plot(histories[j].history['val_acc'])\n",
        "  plt.xticks(np.arange(0, n_epochs+2, 5))\n",
        "  plt.yticks(np.arange(0.3, 1, 0.1))\n",
        "  plt.title(models[j].name)\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SMRLuq06C0Dj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "62eb8d69-ebf8-4a6a-8ea6-bafac09a6d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1656x360 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These plots shows that the accuracy on the validation set is always close the one the training set: it seems that BiGRU overfits more than the other models, but if we print the accuracy of the last epoch (where it seems to be the highest difference between the two accuracies):"
      ],
      "metadata": {
        "id": "5nG_9UnnUgMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy on the training set for BiGRU:', round(histories['BiGRU'].history['acc'][-1],2))\n",
        "print('Accuracy on the validation set for BiGRU:', round(histories['BiGRU'].history['val_acc'][-1],2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dL_YavXU4gH",
        "outputId": "34fbeb91-b8c9-40b5-ed29-a6c872f141f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the training set for BiGRU: 0.96\n",
            "Accuracy on the validation set for BiGRU: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see that there is a difference of only the 3% between the two."
      ],
      "metadata": {
        "id": "SKkKVewjVTmy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYDsfY9cGgrt"
      },
      "source": [
        "## 11. F1 Score for the models on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "_Kpe-CktfwgN"
      },
      "outputs": [],
      "source": [
        "# here there are some print functions (for macro F1 score, classification report, etc)\n",
        "# you can skip this part and see directly the results \n",
        "from numpy import argmax\n",
        "from sklearn.metrics import classification_report\n",
        "from statistics import mean\n",
        "\n",
        "punctuation = [\",\", \".\", \"''\", \":\", \"(\", \")\", \"``\", \"#\", \"{\", \"}\", '$']\n",
        "\n",
        "# we build a single list where appending all the results\n",
        "# ex: if val.shape=100 and max_length=10 (max n. of words in a sentence), the total length is 100*10=1000\n",
        "# y --> the list of lists of one-hot-encoding\n",
        "# max_length --> max n. of words in a sentence\n",
        "def unique_list(y, max_length):\n",
        "  unique_list = []\n",
        "  for i in range(len(y)):\n",
        "    for j in range(max_length):\n",
        "\n",
        "      # first we convert each one-hot-encoding in the integer encoding \n",
        "      argmax = y[i][j].argmax()\n",
        "\n",
        "      # not pad\n",
        "      if argmax > 0:\n",
        "        # we take the word using the encode integer\n",
        "        tag = tag_tokenizer.index_word[argmax]\n",
        "        unique_list.append(tag.upper())\n",
        "\n",
        "      # padding\n",
        "      else:\n",
        "        unique_list.append(\"-PAD-\")\n",
        "  return unique_list\n",
        "\n",
        "\n",
        "# print the macro F1 score without punctuation\n",
        "# print_report --> True if you want to show accuracy and F1 for each class\n",
        "# pad --> True if you want to print the F1 score with the class -PAD-\n",
        "# printF1 --> True if you want to show the macro F1\n",
        "def f1_score(y_true, y_pred, name, print_report = False, pad = False, printF1=True, printClasses=False):\n",
        "  if print_report: print(\"Report for \", name)\n",
        "  report = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
        "  f1 = []\n",
        "  to_remove = [\"macro avg\", \"accuracy\",\"weighted avg\"]\n",
        "  total = 0\n",
        "  count_1 = 0\n",
        "  count_2 = 0\n",
        "  count_3 = 0\n",
        "  if not pad: to_remove.append('-PAD-')\n",
        "  for c in list(report.keys()):\n",
        "    if c in punctuation or c in to_remove: continue\n",
        "    total += 1\n",
        "    f1.append(report[c][\"f1-score\"])\n",
        "    if print_report:\n",
        "      print(\"Class: {0}\\tF1: {1:.3f}\\tSupport: {2}\\tPrecision: {3}\\t\\tRecall: {4}\".format(c, report[c][\"f1-score\"], report[c]['support'], round(report[c][\"precision\"],2), round(report[c][\"recall\"],2)))\n",
        "      if report[c][\"f1-score\"] >= 0.8:\n",
        "        count_1 += 1\n",
        "      elif report[c][\"f1-score\"] >= 0.5 and report[c][\"f1-score\"] < 0.8 :\n",
        "        count_2 += 1\n",
        "      else: count_3 += 1\n",
        "  if printF1:\n",
        "    print(\"The F1-macro for {0} is: {1:.3f}\".format(name, mean(f1)))\n",
        "  if printClasses:\n",
        "    print(\"The percentage of classes with F1 score higher than 0.8 is {:.2%}\".format(round(count_1/total, 2)))\n",
        "    print(\"The percentage of classes with F1 score between 0.5 and 0.8 is {:.2%} \".format(round(count_2/total,2)))\n",
        "    print(\"The percentage of classes with F1 score lower than 0.5 is {:.2%} \".format(round(count_3/total,2)))\n",
        "  if print_report: print()\n",
        "  return mean(f1), report\n",
        "\n",
        "\n",
        "# print the F1 score for the class -PAD-\n",
        "def f1_pad(y_true, y_pred, name):\n",
        "   report = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
        "   for k in list(report.keys()):\n",
        "     if k == '-PAD-':\n",
        "       print('The -PAD- F1 score for {0} is: {1}'.format(name, round(report[k]['f1-score'],3)))\n",
        "\n",
        "\n",
        "# print the report (F1, score) for each class for the OOVs\n",
        "def OOV_report(y_test, y_pred, X_test, name, OOV):\n",
        "  count_true_OOV = 0 # count the number of OOV correctly guessed\n",
        "  total_OOV = 0 # it's not sufficient use the voc OOV length, because a OOV can appear many times\n",
        "  true_OOV = []\n",
        "  pred_OOV = []\n",
        "  for i in range(len(X_test)):\n",
        "    if X_test[i] in OOV:\n",
        "      if y_test[i] == y_pred[i]:\n",
        "        count_true_OOV += 1\n",
        "      pred_OOV.append(y_pred[i])\n",
        "      true_OOV.append(y_test[i])\n",
        "      total_OOV += 1\n",
        "  f1_score(true_OOV, pred_OOV, name=name, print_report=True)\n",
        "  return count_true_OOV/total_OOV*100\n",
        "\n",
        "\n",
        "# print the F1 diff between the left model and the right model (for each class, it prints F1_left - F1_right)\n",
        "def diff_f1(left, right, right_name='Random Embedding', string='context and random embedding'):\n",
        "  print('F1 score difference between {0}:'.format(string))\n",
        "  to_remove = [\"macro avg\", \"weighted avg\", \"accuracy\"]\n",
        "  for i in list(left.keys()):\n",
        "    if i in punctuation or i in to_remove: continue\n",
        "    diff = None\n",
        "    try:\n",
        "      diff = left[i]['f1-score']-right[i]['f1-score']\n",
        "      diff = round(diff, 2)\n",
        "      if diff >= 0: print('Class: {0}\\tF1 diff: \\033[92m+{1}\\033[0m'.format(i, abs(diff)))\n",
        "      else: print('Class: {0}\\tF1 diff: \\033[91m-{1}\\033[0m'.format(i, abs(diff)))\n",
        "    except: # if a class isn't in the right model\n",
        "      diff = left[i]['f1-score']\n",
        "      print('Class: {0}\\tF1 diff: {1} (Not in the {2})'.format(i, round(abs(diff),2), right_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DitucSpa/POS_Tagging_NLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqdZ0f7lJdr2",
        "outputId": "4b7bcee3-4f05-40b8-b96c-5bae2c7c2a0e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'POS_Tagging_NLP'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 36 (delta 19), reused 26 (delta 18), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (36/36), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"POS_Tagging_NLP/training_weights\""
      ],
      "metadata": {
        "id": "gG6dkMgAKGMC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we upload the model weights for each model. We train the model for seed 1, 10 and 6666. In addition, we save the model with seed equal to 10 for showing the F1 score for each tag and the test of the models. "
      ],
      "metadata": {
        "id": "sDQN5RNEBWR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concat_val = unique_list(y_val, max_length)\n",
        "for i in list(models.keys()):\n",
        "  val_mean = []\n",
        "  for seed_loop in [1, 10, 66666]:\n",
        "    models[i].load_weights(checkpoint_path+'/{0}_{1}.hdf5'.format(models[i].name, seed_loop)) # load weights\n",
        "    adam = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
        "    models[i].compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
        "    models[i].evaluate(X_val, y_val, batch_size=32, verbose=0)\n",
        "    y_pred=models[i].predict(X_val, verbose=0)\n",
        "    if seed_loop==10: y_pred_val[i]=y_pred \n",
        "    tmp, _ = f1_score(concat_val, unique_list(y_pred, max_length), name = models[i].name+' without -PAD-', print_report=False, printF1=False)\n",
        "    val_mean.append(tmp)\n",
        "  print('Model: {0}, mean val F1: {1}'.format(models[i].name, round(mean(val_mean),2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01e5hb5g4CBh",
        "outputId": "f126f92f-95ec-45bd-dfa8-df1dd5ea6afa"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: BiLSTM, mean val F1: 0.75\n",
            "Model: Random_BiLSTM, mean val F1: 0.74\n",
            "Model: BiGRU, mean val F1: 0.73\n",
            "Model: BiDense, mean val F1: 0.7\n",
            "Model: ML_BiLSTM, mean val F1: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8rDSqaR6vAW"
      },
      "source": [
        "Let's see the F1 score on the validation set with and without the padding for the random and context embedding with seed=10. In the following box, we show with green values the F1 scores for the classes where the context embedding has a higher/equal values than random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "mTgvbntINPFf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d7958e-bc78-42bd-a4fa-2786cb2f67a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The F1-macro for BiLSTM without -PAD- is: 0.763\n",
            "The F1-macro for Random_BiLSTM without -PAD- is: 0.736\n",
            "F1 score difference between context and random embedding:\n",
            "Class: -PAD-\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: CC\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: CD\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: DT\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: EX\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: FW\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: IN\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: JJ\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: JJR\tF1 diff: \u001b[92m+0.01\u001b[0m\n",
            "Class: JJS\tF1 diff: \u001b[91m-0.07\u001b[0m\n",
            "Class: LS\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: MD\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: NN\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: NNP\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: NNPS\tF1 diff: \u001b[92m+0.04\u001b[0m\n",
            "Class: NNS\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: PDT\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: POS\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: PRP\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: PRP$\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: RB\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: RBR\tF1 diff: \u001b[91m-0.02\u001b[0m\n",
            "Class: RBS\tF1 diff: \u001b[92m+0.35\u001b[0m\n",
            "Class: RP\tF1 diff: \u001b[91m-0.04\u001b[0m\n",
            "Class: TO\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: UH\tF1 diff: \u001b[92m+0.67\u001b[0m\n",
            "Class: VB\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: VBD\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: VBG\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: VBN\tF1 diff: \u001b[92m+0.01\u001b[0m\n",
            "Class: VBP\tF1 diff: \u001b[92m+0.03\u001b[0m\n",
            "Class: VBZ\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: WDT\tF1 diff: \u001b[92m+0.04\u001b[0m\n",
            "Class: WP\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: WP$\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: WRB\tF1 diff: \u001b[91m-0.01\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "concat_val = unique_list(y_val, max_length)\n",
        "diff_f1(f1_score(concat_val, unique_list(y_pred_val['BiLSTM'], max_length), name = models['BiLSTM'].name+' without -PAD-', print_report = False)[1], f1_score(concat_val, unique_list(y_pred_val['Random_BiLSTM'], max_length), name = models['Random_BiLSTM'].name+' without -PAD-', print_report = False)[1] )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, the macro F1 score is higher for the context embedding of 3%. More specifically, we can see that only 10 classes have a higher value for the F1 score for random embedding, and we can also notice that these differences are very little, since the highest one has an absolute value of 0.7. Regarding the classes with an higher F1 score for the context embedding, only for RBS and UH there is an big improvement, while for the others the values are more or less the same.\n",
        "Then, we can conclude that the context embedding has a big impact only for the RBS and UH classes (with an increase of 0.35 and 0.67 respectively), which however reflects an improvement for the F1 score of the context embedding of 3%. Since this embedding has the highest F1 score, we picked this. "
      ],
      "metadata": {
        "id": "FmxpI1eYxi-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k in ['BiLSTM', 'BiGRU', 'BiDense', 'ML_BiLSTM']:\n",
        "  f1_pad(concat_val, unique_list(y_pred_val[k], max_length), models[k].name)\n",
        "  f1 = f1_score(concat_val, unique_list(y_pred_val[k], max_length), name = models[k].name+' without -PAD-', print_report=False)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJELhTqxg8YF",
        "outputId": "bb9b3af8-6f5f-4c8c-e2c6-f8e33b398d19"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The -PAD- F1 score for BiLSTM is: 1.0\n",
            "The F1-macro for BiLSTM without -PAD- is: 0.763\n",
            "\n",
            "The -PAD- F1 score for BiGRU is: 1.0\n",
            "The F1-macro for BiGRU without -PAD- is: 0.722\n",
            "\n",
            "The -PAD- F1 score for BiDense is: 1.0\n",
            "The F1-macro for BiDense without -PAD- is: 0.707\n",
            "\n",
            "The -PAD- F1 score for ML_BiLSTM is: 1.0\n",
            "The F1-macro for ML_BiLSTM without -PAD- is: 0.745\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, each model has correctly leant (with F1 score = 1) the mapping from the zeros of the padding to the class -PAD-. According to the macro F1 score on the validation set, the two best models are the Bi_LSTM and the ML-BiLSTM, which we are now going to evaluate on the test set, by using the weights saved with seed=10."
      ],
      "metadata": {
        "id": "qvUkLabOo4Em"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xaiKBYfo-ej"
      },
      "source": [
        "## 12. Test of BiLSTM model \n",
        "Now we test the BiLSTM model on the test set and prints some results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvqlMN2p4zyc",
        "outputId": "15ec8f2b-3424-4455-b949-0d94086c9321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report for  BiLSTM without -PAD-\n",
            "Class: CC\tF1: 0.994\tSupport: 336\tPrecision: 0.99\t\tRecall: 1.0\n",
            "Class: CD\tF1: 0.978\tSupport: 767\tPrecision: 0.99\t\tRecall: 0.97\n",
            "Class: DT\tF1: 0.987\tSupport: 1269\tPrecision: 0.99\t\tRecall: 0.98\n",
            "Class: EX\tF1: 0.909\tSupport: 5\tPrecision: 0.83\t\tRecall: 1.0\n",
            "Class: IN\tF1: 0.963\tSupport: 1538\tPrecision: 0.95\t\tRecall: 0.97\n",
            "Class: JJ\tF1: 0.806\tSupport: 869\tPrecision: 0.78\t\tRecall: 0.83\n",
            "Class: JJR\tF1: 0.759\tSupport: 57\tPrecision: 0.8\t\tRecall: 0.72\n",
            "Class: JJS\tF1: 0.807\tSupport: 31\tPrecision: 0.88\t\tRecall: 0.74\n",
            "Class: LS\tF1: 0.000\tSupport: 0\tPrecision: 0.0\t\tRecall: 0.0\n",
            "Class: MD\tF1: 0.982\tSupport: 160\tPrecision: 0.96\t\tRecall: 1.0\n",
            "Class: NN\tF1: 0.872\tSupport: 2217\tPrecision: 0.86\t\tRecall: 0.88\n",
            "Class: NNP\tF1: 0.853\tSupport: 1414\tPrecision: 0.89\t\tRecall: 0.82\n",
            "Class: NNPS\tF1: 0.182\tSupport: 39\tPrecision: 0.31\t\tRecall: 0.13\n",
            "Class: NNS\tF1: 0.907\tSupport: 881\tPrecision: 0.89\t\tRecall: 0.93\n",
            "Class: PDT\tF1: 0.000\tSupport: 4\tPrecision: 0.0\t\tRecall: 0.0\n",
            "Class: POS\tF1: 0.959\tSupport: 143\tPrecision: 0.95\t\tRecall: 0.97\n",
            "Class: PRP\tF1: 0.995\tSupport: 184\tPrecision: 0.99\t\tRecall: 1.0\n",
            "Class: PRP$\tF1: 0.995\tSupport: 94\tPrecision: 0.99\t\tRecall: 1.0\n",
            "Class: RB\tF1: 0.865\tSupport: 360\tPrecision: 0.88\t\tRecall: 0.85\n",
            "Class: RBR\tF1: 0.343\tSupport: 14\tPrecision: 0.29\t\tRecall: 0.43\n",
            "Class: RBS\tF1: 0.429\tSupport: 3\tPrecision: 0.27\t\tRecall: 1.0\n",
            "Class: RP\tF1: 0.615\tSupport: 33\tPrecision: 0.48\t\tRecall: 0.85\n",
            "Class: TO\tF1: 0.999\tSupport: 366\tPrecision: 1.0\t\tRecall: 1.0\n",
            "Class: VB\tF1: 0.910\tSupport: 379\tPrecision: 0.9\t\tRecall: 0.92\n",
            "Class: VBD\tF1: 0.880\tSupport: 604\tPrecision: 0.87\t\tRecall: 0.89\n",
            "Class: VBG\tF1: 0.837\tSupport: 210\tPrecision: 0.84\t\tRecall: 0.83\n",
            "Class: VBN\tF1: 0.717\tSupport: 350\tPrecision: 0.76\t\tRecall: 0.68\n",
            "Class: VBP\tF1: 0.810\tSupport: 130\tPrecision: 0.84\t\tRecall: 0.78\n",
            "Class: VBZ\tF1: 0.921\tSupport: 271\tPrecision: 0.92\t\tRecall: 0.92\n",
            "Class: WDT\tF1: 0.809\tSupport: 79\tPrecision: 0.96\t\tRecall: 0.7\n",
            "Class: WP\tF1: 1.000\tSupport: 18\tPrecision: 1.0\t\tRecall: 1.0\n",
            "Class: WP$\tF1: 1.000\tSupport: 2\tPrecision: 1.0\t\tRecall: 1.0\n",
            "Class: WRB\tF1: 1.000\tSupport: 23\tPrecision: 1.0\t\tRecall: 1.0\n",
            "The F1-macro for BiLSTM without -PAD- is: 0.790\n",
            "The percentage of classes with F1 score higher than 0.8 is 76.00%\n",
            "The percentage of classes with F1 score between 0.5 and 0.8 is 9.00% \n",
            "The percentage of classes with F1 score lower than 0.5 is 15.00% \n",
            "\n"
          ]
        }
      ],
      "source": [
        "models['BiLSTM'].load_weights(checkpoint_path+'/BiLSTM_10.hdf5')\n",
        "y_pred_bi_lstm = models['BiLSTM'].predict(X_test, verbose=verbose)\n",
        "y_pred_bi_lstm = unique_list(y_pred_bi_lstm, max_length)\n",
        "y_test_bi_lstm = unique_list(y_test, max_length)\n",
        "_,_ = f1_score(y_test_bi_lstm, y_pred_bi_lstm, name='BiLSTM without -PAD-', print_report=True, pad=False, printClasses=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance on the test set in 0.79, which is higher than the one on the validation set, 0.76. To explore more in detail the reasons of this behaviour,let's compare the performance of the model (BiLSTM) for each class, on the validation and test splits. The green results are better F1 scores for the validation set, while the red results for the test split."
      ],
      "metadata": {
        "id": "L4tdYTBRIZC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diff_f1(f1_score(concat_val, unique_list(y_pred_val['BiLSTM'], max_length), name = models['BiLSTM'].name+' without -PAD- for VAL', print_report = False)[1], f1_score(y_test_bi_lstm, y_pred_bi_lstm, name = models['BiLSTM'].name+' without -PAD- for TEST', print_report = False)[1], right_name='Test', string='validation and test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldzgAFfO2Xxb",
        "outputId": "22123817-71c8-4901-b64e-8990da1d1dc3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The F1-macro for BiLSTM without -PAD- for VAL is: 0.763\n",
            "The F1-macro for BiLSTM without -PAD- for TEST is: 0.790\n",
            "F1 score difference between validation and test:\n",
            "Class: -PAD-\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: CC\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: CD\tF1 diff: \u001b[91m-0.02\u001b[0m\n",
            "Class: DT\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: EX\tF1 diff: \u001b[92m+0.04\u001b[0m\n",
            "Class: FW\tF1 diff: 0.0 (Not in the Test)\n",
            "Class: IN\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: JJ\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: JJR\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: JJS\tF1 diff: \u001b[91m-0.06\u001b[0m\n",
            "Class: LS\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: MD\tF1 diff: \u001b[92m+0.01\u001b[0m\n",
            "Class: NN\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: NNP\tF1 diff: \u001b[91m-0.02\u001b[0m\n",
            "Class: NNPS\tF1 diff: \u001b[92m+0.04\u001b[0m\n",
            "Class: NNS\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: PDT\tF1 diff: \u001b[92m+0.11\u001b[0m\n",
            "Class: POS\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: PRP\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: PRP$\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: RB\tF1 diff: \u001b[92m+0.01\u001b[0m\n",
            "Class: RBR\tF1 diff: \u001b[92m+0.08\u001b[0m\n",
            "Class: RBS\tF1 diff: \u001b[92m+0.18\u001b[0m\n",
            "Class: RP\tF1 diff: \u001b[91m-0.13\u001b[0m\n",
            "Class: TO\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: UH\tF1 diff: 0.67 (Not in the Test)\n",
            "Class: VB\tF1 diff: \u001b[91m-0.05\u001b[0m\n",
            "Class: VBD\tF1 diff: \u001b[91m-0.06\u001b[0m\n",
            "Class: VBG\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: VBN\tF1 diff: \u001b[91m-0.02\u001b[0m\n",
            "Class: VBP\tF1 diff: \u001b[92m+0.01\u001b[0m\n",
            "Class: VBZ\tF1 diff: \u001b[92m+0.01\u001b[0m\n",
            "Class: WDT\tF1 diff: \u001b[91m-0.09\u001b[0m\n",
            "Class: WP\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: WP$\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: WRB\tF1 diff: \u001b[91m-0.01\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, half of the classes are better for the validation set, the other half for the test split, even if the differences are very little. What affects the final F1 score is that in the test set there are two classes that don't appear (UH and FW, as shown in the chapter '4. Some Statistics'). Besides, their F1 scores are very low: these two values decrease the macro F1 score of the validation split wrt the test set. Indeed, if we print the F1 score for Test and Val without the classes UH and FW we obtain the same macro F1 score."
      ],
      "metadata": {
        "id": "Ey5lCuT14u9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = [\",\", \".\", \"''\", \":\", \"(\", \")\", \"``\", \"#\", \"{\", \"}\", '$', 'UH', 'FW']\n",
        "_ = f1_score(concat_val, unique_list(y_pred_val['BiLSTM'], max_length), name = models['BiLSTM'].name+' without -PAD- for VAL', print_report = False)[0]\n",
        "_ = f1_score(y_test_bi_lstm, y_pred_bi_lstm, name = models['BiLSTM'].name+' without -PAD- for TEST', print_report = False)[0]\n",
        "punctuation = [\",\", \".\", \"''\", \":\", \"(\", \")\", \"``\", \"#\", \"{\", \"}\", '$']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlt4nrX8BEDN",
        "outputId": "0088a8b8-3217-49e4-dc4b-a78f1dbce447"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The F1-macro for BiLSTM without -PAD- for VAL is: 0.789\n",
            "The F1-macro for BiLSTM without -PAD- for TEST is: 0.790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "C4znVehOBd2f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6deLYFur61cs"
      },
      "source": [
        "## 13. Test of the ML-BiLSTM\n",
        "Now we test the BiLSTM model on the test set and prints some results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuGecFAr-w1j",
        "outputId": "b9c7989c-9c69-470e-d031-1ea8014eb0ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report for  ML-BiLSTM without -PAD-\n",
            "Class: CC\tF1: 0.994\tSupport: 336\tPrecision: 0.99\t\tRecall: 1.0\n",
            "Class: CD\tF1: 0.974\tSupport: 767\tPrecision: 0.98\t\tRecall: 0.97\n",
            "Class: DT\tF1: 0.987\tSupport: 1269\tPrecision: 0.99\t\tRecall: 0.98\n",
            "Class: EX\tF1: 0.909\tSupport: 5\tPrecision: 0.83\t\tRecall: 1.0\n",
            "Class: IN\tF1: 0.960\tSupport: 1538\tPrecision: 0.95\t\tRecall: 0.97\n",
            "Class: JJ\tF1: 0.803\tSupport: 869\tPrecision: 0.78\t\tRecall: 0.83\n",
            "Class: JJR\tF1: 0.835\tSupport: 57\tPrecision: 0.83\t\tRecall: 0.84\n",
            "Class: JJS\tF1: 0.759\tSupport: 31\tPrecision: 0.81\t\tRecall: 0.71\n",
            "Class: MD\tF1: 0.981\tSupport: 160\tPrecision: 0.97\t\tRecall: 0.99\n",
            "Class: NN\tF1: 0.873\tSupport: 2217\tPrecision: 0.87\t\tRecall: 0.88\n",
            "Class: NNP\tF1: 0.866\tSupport: 1414\tPrecision: 0.89\t\tRecall: 0.84\n",
            "Class: NNPS\tF1: 0.089\tSupport: 39\tPrecision: 0.33\t\tRecall: 0.05\n",
            "Class: NNS\tF1: 0.904\tSupport: 881\tPrecision: 0.88\t\tRecall: 0.93\n",
            "Class: PDT\tF1: 0.000\tSupport: 4\tPrecision: 0.0\t\tRecall: 0.0\n",
            "Class: POS\tF1: 0.931\tSupport: 143\tPrecision: 0.96\t\tRecall: 0.9\n",
            "Class: PRP\tF1: 0.997\tSupport: 184\tPrecision: 1.0\t\tRecall: 0.99\n",
            "Class: PRP$\tF1: 0.995\tSupport: 94\tPrecision: 0.99\t\tRecall: 1.0\n",
            "Class: RB\tF1: 0.836\tSupport: 360\tPrecision: 0.85\t\tRecall: 0.82\n",
            "Class: RBR\tF1: 0.429\tSupport: 14\tPrecision: 0.43\t\tRecall: 0.43\n",
            "Class: RBS\tF1: 0.308\tSupport: 3\tPrecision: 0.2\t\tRecall: 0.67\n",
            "Class: RP\tF1: 0.544\tSupport: 33\tPrecision: 0.4\t\tRecall: 0.85\n",
            "Class: TO\tF1: 1.000\tSupport: 366\tPrecision: 1.0\t\tRecall: 1.0\n",
            "Class: VB\tF1: 0.844\tSupport: 379\tPrecision: 0.86\t\tRecall: 0.83\n",
            "Class: VBD\tF1: 0.869\tSupport: 604\tPrecision: 0.88\t\tRecall: 0.86\n",
            "Class: VBG\tF1: 0.768\tSupport: 210\tPrecision: 0.76\t\tRecall: 0.77\n",
            "Class: VBN\tF1: 0.705\tSupport: 350\tPrecision: 0.7\t\tRecall: 0.71\n",
            "Class: VBP\tF1: 0.808\tSupport: 130\tPrecision: 0.86\t\tRecall: 0.76\n",
            "Class: VBZ\tF1: 0.919\tSupport: 271\tPrecision: 0.9\t\tRecall: 0.94\n",
            "Class: WDT\tF1: 0.829\tSupport: 79\tPrecision: 0.95\t\tRecall: 0.73\n",
            "Class: WP\tF1: 1.000\tSupport: 18\tPrecision: 1.0\t\tRecall: 1.0\n",
            "Class: WP$\tF1: 0.667\tSupport: 2\tPrecision: 1.0\t\tRecall: 0.5\n",
            "Class: WRB\tF1: 0.978\tSupport: 23\tPrecision: 1.0\t\tRecall: 0.96\n",
            "The F1-macro for ML-BiLSTM without -PAD- is: 0.792\n",
            "The percentage of classes with F1 score higher than 0.8 is 72.00%\n",
            "The percentage of classes with F1 score between 0.5 and 0.8 is 16.00% \n",
            "The percentage of classes with F1 score lower than 0.5 is 12.00% \n",
            "\n"
          ]
        }
      ],
      "source": [
        "models['ML_BiLSTM'].load_weights(checkpoint_path+'/ML_BiLSTM_10.hdf5')\n",
        "y_pred_ML_bi_lstm = unique_list(models['ML_BiLSTM'].predict(X_test, verbose=verbose), max_length)\n",
        "y_test_ML_bi_lstm = unique_list(y_test, max_length)\n",
        "_,_ = f1_score(y_test_ML_bi_lstm, y_pred_ML_bi_lstm, name='ML-BiLSTM without -PAD-', print_report=True, pad=False, printClasses=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the performance on the test set is 0.79, which is higher than the one on the validation set, 0.75. As we have done for the previous model, let's compare the performance of the model (ML-BiLSTM) on the validation and test splits. The green results are better F1 scores for the validation set, while the red results for the test split."
      ],
      "metadata": {
        "id": "BGaAVRvDJQwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diff_f1(f1_score(concat_val, unique_list(y_pred_val['ML_BiLSTM'], max_length), name = models['ML_BiLSTM'].name+' without -PAD- for VAL', print_report = False)[1], f1_score(y_test_bi_lstm, y_pred_bi_lstm, name = models['ML_BiLSTM'].name+' without -PAD- for TEST', print_report = False)[1], right_name='Test', string='validation and test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tstbS1C3iES",
        "outputId": "78bcb14f-2b6a-4990-a844-b839ad48cd97"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The F1-macro for ML_BiLSTM without -PAD- for VAL is: 0.745\n",
            "The F1-macro for ML_BiLSTM without -PAD- for TEST is: 0.790\n",
            "F1 score difference between validation and test:\n",
            "Class: -PAD-\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: CC\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: CD\tF1 diff: \u001b[91m-0.02\u001b[0m\n",
            "Class: DT\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: EX\tF1 diff: \u001b[92m+0.04\u001b[0m\n",
            "Class: FW\tF1 diff: 0.0 (Not in the Test)\n",
            "Class: IN\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: JJ\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: JJR\tF1 diff: \u001b[92m+0.03\u001b[0m\n",
            "Class: JJS\tF1 diff: \u001b[91m-0.05\u001b[0m\n",
            "Class: MD\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: NN\tF1 diff: \u001b[91m-0.02\u001b[0m\n",
            "Class: NNP\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: NNPS\tF1 diff: \u001b[91m-0.12\u001b[0m\n",
            "Class: NNS\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: PDT\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: POS\tF1 diff: \u001b[92m+0.01\u001b[0m\n",
            "Class: PRP\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: PRP$\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: RB\tF1 diff: \u001b[91m-0.02\u001b[0m\n",
            "Class: RBR\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: RBS\tF1 diff: \u001b[92m+0.17\u001b[0m\n",
            "Class: RP\tF1 diff: \u001b[91m-0.1\u001b[0m\n",
            "Class: TO\tF1 diff: \u001b[92m+0.0\u001b[0m\n",
            "Class: UH\tF1 diff: 0.0 (Not in the Test)\n",
            "Class: VB\tF1 diff: \u001b[91m-0.18\u001b[0m\n",
            "Class: VBD\tF1 diff: \u001b[91m-0.07\u001b[0m\n",
            "Class: VBG\tF1 diff: \u001b[91m-0.04\u001b[0m\n",
            "Class: VBN\tF1 diff: \u001b[91m-0.02\u001b[0m\n",
            "Class: VBP\tF1 diff: \u001b[91m-0.05\u001b[0m\n",
            "Class: VBZ\tF1 diff: \u001b[92m+0.01\u001b[0m\n",
            "Class: WDT\tF1 diff: \u001b[91m-0.09\u001b[0m\n",
            "Class: WP\tF1 diff: \u001b[91m-0.01\u001b[0m\n",
            "Class: WP$\tF1 diff: \u001b[91m-0.2\u001b[0m\n",
            "Class: WRB\tF1 diff: \u001b[92m+0.0\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the macro F1 score of the validation is influenced by the classes UH and FW, as the previous model. Note that, for ML-BiLSTM, the F1 score for UH is worse than the UH's F1 score for BiLSTM."
      ],
      "metadata": {
        "id": "Rlf9bsGpB290"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = [\",\", \".\", \"''\", \":\", \"(\", \")\", \"``\", \"#\", \"{\", \"}\", '$', 'UH', 'FW']\n",
        "_ = f1_score(concat_val, unique_list(y_pred_val['ML_BiLSTM'], max_length), name = models['ML_BiLSTM'].name+' without -PAD- for VAL', print_report = False)[0]\n",
        "_ = f1_score(y_test_ML_bi_lstm, y_pred_ML_bi_lstm, name = models['ML_BiLSTM'].name+' without -PAD- for TEST', print_report = False)[0]\n",
        "punctuation = [\",\", \".\", \"''\", \":\", \"(\", \")\", \"``\", \"#\", \"{\", \"}\", '$']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjt4HIMQBjl9",
        "outputId": "0db9f07e-87ea-4afe-ae9e-1b3e3e542a79"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The F1-macro for ML_BiLSTM without -PAD- for VAL is: 0.791\n",
            "The F1-macro for ML_BiLSTM without -PAD- for TEST is: 0.792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. Bibliography and sitography\n",
        "\n",
        "- [Word Embedding and GloVe, Towards Data Science](https://medium.com/@jonathan-hui/nlp-word-embedding-glove-5e7f523999f6);\n",
        "- [Handling OOV, Towards Data Science](https://medium.com/analytics-vidhya/handling-out-of-vocabulary-words-in-natural-language-processing-based-on-context-4bbba16214d5);\n",
        "- [LSTM and GloVe, Towards Data Science](https://medium.com/towards-data-science/classify-toxic-online-comments-with-lstm-and-glove-e455a58da9c7);\n",
        "- [Padding and Masking, Towards Data Science](https://medium.com/@shailaja21/preprocessing-sequence-data-in-keras-padding-masking-techniques-aa087fe7319c);\n",
        "- [Multi-Class Text Classification with LSTM, Towards Data Science](https://medium.com/towards-data-science/multi-class-text-classification-with-lstm-1590bee1bd17);\n",
        "- [Units in LSTM, Coding Notes](https://tung2389.github.io/coding-note/unitslstm);\n",
        "- [Difference between LSTM recurrent dropout and dropout, StackOverflow](https://stackoverflow.com/questions/44924690/keras-the-difference-between-lstm-dropout-and-lstm-recurrent-dropout);\n",
        "- [GloVe Official Website](https://nlp.stanford.edu/projects/glove/);\n",
        "- [Scikit-Learn Official Website](https://scikit-learn.org/stable/);\n",
        "- [Keras Official Website](https://keras.io).\n"
      ],
      "metadata": {
        "id": "_BLtWrjo1N3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GitHub Repository\n",
        "You can find the PDF, the notebook and the model weights on:\n",
        "https://github.com/DitucSpa/POS_Tagging_NLP"
      ],
      "metadata": {
        "id": "tu4UehmoO4jy"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "g10GGVj9hlFH",
        "jY0MAkV_8PlE",
        "nH089Sbn8a-f",
        "ZZrLvFqj2a-a",
        "FBvJbVok8uh0",
        "KxyVGB5b9fA6",
        "QE126aDp4TIY",
        "UQQpDqgI4bE-",
        "1N0ZIFLez182",
        "DFx5EJGU4BWo",
        "CYDsfY9cGgrt",
        "-xaiKBYfo-ej",
        "6deLYFur61cs"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}